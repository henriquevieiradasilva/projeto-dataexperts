{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6b66b5-6346-4a61-a6bb-26e5229b10ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuração final da estrutura de diretórios\n",
    "\n",
    "* data_ex (catalog) \n",
    "    * metadados (schema)\n",
    "        * meta_bronze (table)\n",
    "        * meta_silver (table)\n",
    "        * meta_gold (table)\n",
    "    * lhdw (schema) \n",
    "        * versao* (volumes)       \n",
    "    * bronze (schema)\n",
    "        * versao* (volume)\n",
    "            * bronze_dim_categoria_produto\n",
    "            * bronze_dim_cliente \n",
    "            * bronze_dim_data \n",
    "            * bronze_dim_localidade\n",
    "            * bronze_dim_produto \n",
    "            * bronze_fato_vendas  \n",
    "    * silver (schema)\n",
    "        * versao* (volume)\n",
    "            * silver_dim_categoria_produto\n",
    "            * silver_dim_cliente \n",
    "            * silver_dim_data \n",
    "            * silver_dim_localidade\n",
    "            * silver_dim_produto \n",
    "            * silver_fato_vendas\n",
    "    * gold  (schema)\n",
    "        * versao* (volume)\n",
    "            * dim_categoria_produto\n",
    "            * dim_cliente \n",
    "            * dim_data \n",
    "            * dim_localidade\n",
    "            * dim_produto \n",
    "            * fato_vendas\n",
    "\n",
    "Tornar o escrito a cima em um diagrama para ser um dos anexos ao storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691f7456-a8b0-470d-8835-3b96f64beab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Registro de Logs de Carga\n",
    "* id_carga (string (uuid))\n",
    "* id_job (string (uuid))\n",
    "* nome_arquivo (string)\n",
    "* fonte (string)\n",
    "* camada (string)\n",
    "* path_destino (string)\n",
    "* data_inicio (utc_timestamp)\n",
    "* data_fim (utc_timestamp)\n",
    "* duracao_ms (bigint)\n",
    "* registros_lidos (bigint)\n",
    "* registros_gravados (bigint)\n",
    "* status (string)\n",
    "* mensagem_erro (string)\n",
    "* data_execucao (date(yyyy-MM-dd))\n",
    "\n",
    "## Exemplo\n",
    "| Campo                  | Valor                                         |\n",
    "| ---------------------- | --------------------------------------------- |\n",
    "| **id_carga**           | `8f2c3b9e-4d7a-4e91-b0c1-9a5f7c6d2e11`        |\n",
    "| **id_job**             | `1a6b4c20-3c77-4c0e-9f5a-8b12f9e3a001`        |\n",
    "| **nome_arquivo**       | `cliente2.csv`                                |\n",
    "| **fonte**              | `FILESYSTEM_LOCAL`                            |\n",
    "| **camada**             | `bronze`                                      |\n",
    "| **path_destino**       | `/Volumes/workspace/bronze/clientes/cliente2` |\n",
    "| **data_inicio**        | `2026-02-01 10:15:03.421 UTC`                 |\n",
    "| **data_fim**           | `2026-02-01 10:15:05.987 UTC`                 |\n",
    "| **duracao_ms**         | `2566`                                        |\n",
    "| **registros_lidos**    | `12450`                                       |\n",
    "| **registros_gravados** | `12450`                                       |\n",
    "| **status**             | `SUCCESS`                                     |\n",
    "| **mensagem_erro**      | `NULL`                                        |\n",
    "| **data_execucao**      | `2026-02-01`                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645a6909-75a7-4de7-a0b3-b04a7ac507ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"create catalog if not exists data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"create schema if not exists data_ex.metadados\")\n",
    "spark.sql(\"create schema if not exists data_ex.lhdw\")\n",
    "spark.sql(\"create schema if not exists data_ex.bronze\")\n",
    "spark.sql(\"create schema if not exists data_ex.silver\")\n",
    "spark.sql(\"create schema if not exists data_ex.gold\")\n",
    "\n",
    "# Criação das tabelas de metadados fixos\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_bronze (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_silver (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_gold  (\n",
    "    id_job string\n",
    ") using delta \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c4cf92-ad08-4b35-b1bf-11f1bfe2f7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#spark.sql(\"drop schema if  exists data_ex.lhdw cascade\")\n",
    "#spark.sql(\"drop schema if  exists data_ex.bronze cascade\")\n",
    "spark.sql(\"drop schema if  exists data_ex.metadados cascade\")\n",
    "#spark.sql(\"drop schema if  exists data_ex.silver cascade\")\n",
    "#spark.sql(\"drop schema if  exists data_ex.gold cascade\")\n",
    "# spark.sql(\"drop table if exists data_ex.metadados.meta_bronze\")\n",
    "# spark.sql(\"drop table if exists data_ex.metadados.meta_silver\")\n",
    "# spark.sql(\"drop table if exists data_ex.metadados.meta_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5052501-24b1-43a9-9edf-57cffe33b637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inserindo metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3ebdbe-7012-4c52-9a4a-eab89d8d58fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_job_bronze = str(uuid.uuid4())\n",
    "id_job_silver = str(uuid.uuid4())\n",
    "id_job_gold = str(uuid.uuid4())\n",
    "\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_bronze values ('{id_job_bronze}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_silver values ('{id_job_silver}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_gold values ('{id_job_gold}')\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6034bff5-2783-4bdc-9fee-9a572e05c546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from data_ex.metadados.meta_bronze\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2127ba-495c-4f5b-a93b-0abb97393980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Importação pelo Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca79ccab-b54b-4fa0-b2ac-74170697df69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9734b694-5f90-466a-b2a8-bf2617fae681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "try:\n",
    "   #volumes = spark.sql(\"\"\"select count(volumes) from data_ex.lhdw\"\"\")\n",
    "   volumes = spark.sql(\"show volumes in data_ex.lhdw\")\n",
    "   print(volumes.show())\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível acessar o schema data_ex.lhdw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a888e6b1-f8c9-4dc4-9ae3-67da3ec6c184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Função para baixar os arquivos\n",
    "def download_dataset(csv_files, url, lhdw_path):\n",
    "    # Tenta realizar o download dos arquivos\n",
    "    try :\n",
    "        # Variavel para armazenar o nome do novo volume\n",
    "        volume = \"\"\n",
    "        try:\n",
    "            # Conta quantos volumes já existem dentro desse do schema\n",
    "            numero = spark.sql(\"show volumes in data_ex.lhdw\").count()\n",
    "            # Cria o comando sql para criar um novo volume\n",
    "            volume = f\"download{numero+1:03d}\"\n",
    "            sql_command = f\"create volume if not exists data_ex.lhdw.{volume}\"\n",
    "            # Executa o comando sql acima\n",
    "            spark.sql(sql_command)\n",
    "        # lanca um execao caso houver um erro\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar o volume: {e}\")\n",
    "        for file in csv_files:\n",
    "            down_path = f\"{url+file}\"\n",
    "            up_path = f\"{lhdw_path+volume}/{file}\"\n",
    "            urllib.request.urlretrieve(down_path, up_path)\n",
    "            print(f\"Csv {file} baixado com sucesso!\")\n",
    "    # Caso não consigua realizar o download, uma exceção é lançada\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {file} - {e}\")\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec6e163-cc28-4882-9b7d-f11f09304eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilizaçao da função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378baa36-81fe-4034-be13-8b17aa6662b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de .csv\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Reposotório do dataset para o desafio\n",
    "url = \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\"\n",
    "\n",
    "# Path para o schema de armazenamento, sem o volume\n",
    "lhdw_path = \"/Volumes/data_ex/lhdw/\"\n",
    "\n",
    "volume_salvo = download_dataset(csv_files, url, lhdw_path)\n",
    "print(volume_salvo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cba06a-65ce-4cd7-a220-4d6ecced20fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83da5fa-bf55-4a4f-912c-9511344f507c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do servidor para trabalho no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f314e448-b30c-4b6a-9e40-1bc15c2a95bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f210f36-19a1-4e08-b5f2-95d047b606c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trabalhos no Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8431d0-aab7-4b64-b34a-fee28e8c78ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Variáveis para leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39fc985-77ff-4719-adb4-8944b0dbe401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas/funções necessárias\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType\n",
    "\n",
    "# Definição dos diretorios de trabalho\n",
    "lhdw_path = f\"/Volumes/data_ex/lhdw/{volume_salvo}\"\n",
    "sql_command = f\"create volume if not exists data_ex.bronze.{volume_salvo}\"\n",
    "# Criação do schema bronze\n",
    "spark.sql(sql_command)\n",
    "# Definição do path bronze\n",
    "bronze_path = f\"/Volumes/data_ex/bronze/{volume_salvo}\"\n",
    "\n",
    "# Arquivos csv a serem trabalhados\n",
    "\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Lista dos nomes dos schemas em ordem de execução\n",
    "schemas = []\n",
    "\n",
    "\n",
    "# Definição manual do schemas a serem aplicados nos arquivos csv (futuros parquets)\n",
    "schema_categoria = StructType([\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True),\n",
    "])\n",
    "schemas.append(schema_categoria)\n",
    "\n",
    "schema_cliente = StructType([\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"nome_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_cliente)\n",
    "\n",
    "schema_data = StructType([\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"data\", DateType(), True),\n",
    "    StructField(\"ano\", IntegerType(), True),\n",
    "    StructField(\"mes\", IntegerType(), True),\n",
    "    StructField(\"dia\", IntegerType(), True),\n",
    "    StructField(\"dia_semana\", StringType(), True), \n",
    "    StructField(\"final_de_semana\", BooleanType(), True)\n",
    "])\n",
    "schemas.append(schema_data)\n",
    "\n",
    "schema_localidade = StructType([\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    ])  \n",
    "schemas.append(schema_localidade)\n",
    "\n",
    "schema_produto = StructType([\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_produto)\n",
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"venda_id\", LongType(), True),\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"quantidade\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True)\n",
    "])\n",
    "schemas.append(schema_vendas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8329c678-272a-46e5-aa4c-bd382980c52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Função de Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2150e576-d26b-4429-a8da-0ab472fe8090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando uma lista para armazenar os Sparks DataFrames\n",
    "spark_df = []\n",
    "\n",
    "# Criação do Spark DataFrame, leitura dos csv, aplicação dos schemas e escrita em parquet \n",
    "def bronze_read(csv_files, schemas):\n",
    "    for file, schema in zip(csv_files, schemas):\n",
    "        # Tenta ler os arquivos csv em lhdw_path\n",
    "        try:\n",
    "            # Se schema não for vendas, aplica o schema correspondente a file\n",
    "            if schema != schema_vendas:\n",
    "                df = spark.read\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(schema)\\\n",
    "                    .csv(f\"{lhdw_path}/{file}\")\n",
    "                print(f\"Leitura do arquivo {file} bem sucedida.\")\n",
    "                spark_df.append(df)\n",
    "\n",
    "            # Se for vendas, deve repetir a leirura do arquivo n vezes\n",
    "            else:\n",
    "                # Tenta ler o caminho no qual vendas esta\n",
    "                try:\n",
    "                    paths = dbutils.fs.ls(lhdw_path)\n",
    "                    # Conta o numero csv vendas existentes\n",
    "                    n = len([n for n in paths if n.name.startswith(\"vendas\")])        \n",
    "                    # Aplica um loop para ler todos os arquivos vendas\n",
    "                    for i in range(1, n+1):\n",
    "                        try:\n",
    "                            df = spark.read.option(\"header\", \"true\")\\\n",
    "                                .schema(schema_vendas)\\\n",
    "                                .csv(f\"{lhdw_path}/vendas_part{i}.csv\")\n",
    "                            spark_df.append(df)\n",
    "                            print(f\"Leitura do arquivo vendas_part{i}.csv bem sucedida.\")\n",
    "                           \n",
    "                        except Exception as e:\n",
    "                            print(f\"Erro ao ler o arquivo {lhdw_path}/vendas_part{i}.csv: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(\"ERRO\")\n",
    "                    print(f\"Erro ao ler o caminho no qual no qual vendas esta {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "\n",
    "bronze_read(csv_files, schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff77e76-9a2c-4406-bc54-cd4fcd7cfc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvar no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9f228c-4a0e-41b9-abe4-3002f44e886c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para salvar os arquivos na camada bronze\n",
    "def save(spark_df, camada, dime, fato, n_fatos):\n",
    "    # Variaveis auxiliares\n",
    "    n_dims = len(dime)# conta o numero de dimensões \n",
    "    indice_fato = 0              \n",
    "    \n",
    "    # Loop para criar as dimensões\n",
    "    for i in range(n_dims):\n",
    "        try:\n",
    "            spark_df[i].write.format(\"delta\").mode(\"overwrite\").save(f\"{camada}/{dime[i]}\")\n",
    "            indice_fato = indice_fato + 1\n",
    "            print(f\"DataFrame salvo com sucesso em {camada}/{dime[i]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe: {e}\")\n",
    "\n",
    "    #display(spark_df[indice_fato])\n",
    "    #Loop para criar a tabela de fato vendas\n",
    "    for i in range(0, n_fatos):\n",
    "        try:\n",
    "            spark_df[indice_fato].write.format(\"delta\").mode(\"append\").save(f\"{camada}/{fato}\")\n",
    "            indice_fato = indice_fato + 1\n",
    "            print(f\"DataFrame salvo com sucesso em {camada}/{fato}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe vendas: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410092a0-7455-49e8-adde-47b85cc49ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição do nomes das dimensões a serem salvas\n",
    "dim = [\n",
    "  \"bronze_dim_categoria_produto\",\n",
    "  \"bronze_dim_cliente\",\n",
    "  \"bronze_dim_data\", \n",
    "  \"bronze_dim_localidade\",\n",
    "  \"bronze_dim_produto\", \n",
    "]\n",
    "fato = \"bronze_fato_vendas\"\n",
    "\n",
    "n_fatos = 4\n",
    "\n",
    "# Chamada da função de salvamento\n",
    "save(spark_df, bronze_path, dim, fato, n_fatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4325cac1-f79c-4a41-978e-b33016cdbf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/data_ex/bronze/download001/bronze_fato_vendas\").count()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de682b3-9be7-4aa9-876e-7e920134be33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limpar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749206e6-356b-4ec1-b63c-f05ba0a5f7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
