{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d24b12e-3460-481b-b516-df476f060ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 2 - IMPORTAÇÃO DAS BASES DE DADOS**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora será feito a importação dos dados para a camada especificada do Data LakeHouse que já está pronta.\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bd91bb-80cd-4c6d-afbf-1b92b957b134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "import urllib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7107f0-f8f0-4554-b820-f26b8d9a8fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Importação dos Arquivos para o Data Lakehouse**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse destino` <br>\n",
    "\n",
    "`--> nome_camada --> o nome da camada destino dos dados totalmente brutos` <br>\n",
    "`--> nome_volume_atual --> nome do volume onde ficaram os dados atualizados` <br>\n",
    "`--> nome_volume_historico --> nome do volume onde ficaram o histórico dos dados` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb457114-ee3f-4b63-b5dc-f654ce7ef169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "\n",
    "nome_camada = \"landing_zone\"\n",
    "nome_volume_atual = \"vendas_atual\"\n",
    "nome_volume_historico = \"vendas_historico\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b1381c-6700-48ec-9e73-6acfb47c7bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria os volumes se eles ainda não existirem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e5b2b4-de82-4ddf-928f-565292da130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_volume_atual}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_volume_historico}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef66d7f4-e361-4584-8094-5fa20f716252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira nos vetores todos os nomes dos arquivos que deseja importar e a origem deles, seguindo a mesma ordem:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4464dd7-5d61-4343-bbcc-79ac3e2c43e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "origem_arquivos = [\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e12fac-e969-441e-ab70-bd32548dce58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir verifica se o arquivo já existe no Data Lakehouse:\n",
    "- Se sim, apenas pula para o próximo download (se ainda exitir arquivos na fila em espera)\n",
    "- Se não, tenta baixar o arquivo da origem.\n",
    "Logo após, caso o download dê certo ou não, passa para a próximo download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd566461-ff7e-446c-8a3f-4d535831c7e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "destino_atual = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume_atual}/\"\n",
    "destino_historico = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume_historico}/\"\n",
    "\n",
    "for arquivo, origem in zip(arquivos, origem_arquivos):\n",
    "    entidade = arquivo.replace(\".csv\", \"\")\n",
    "    \n",
    "    try:\n",
    "        arquivos_no_volume = dbutils.fs.ls(destino_atual)\n",
    "        \n",
    "        for arq in arquivos_no_volume:\n",
    "            if arq.name.startswith(f\"{entidade}_\") and arq.name.endswith(\".csv\"):\n",
    "                \n",
    "                nome_sem_extensao = arq.name.replace(\".csv\", \"\")\n",
    "                partes = nome_sem_extensao.split(\"_\")\n",
    "                data_str = partes[-2] \n",
    "                \n",
    "                ano_arq = data_str[0:4]\n",
    "                mes_arq = data_str[4:6]\n",
    "                dia_arq = data_str[6:8]\n",
    "                \n",
    "                pasta_historico_real = f\"{destino_historico}{entidade}/{ano_arq}/{mes_arq}/{dia_arq}/\"\n",
    "                \n",
    "                dbutils.fs.mkdirs(pasta_historico_real)\n",
    "                \n",
    "                caminho_historico = f\"{pasta_historico_real}{arq.name}\"\n",
    "                dbutils.fs.mv(arq.path, caminho_historico)\n",
    "                \n",
    "                print(f\"[INFO] Histórico criado e arquivo movido: {caminho_historico}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    horario_brasil = datetime.timezone(datetime.timedelta(hours=-3))\n",
    "    now = datetime.datetime.now(datetime.timezone.utc).astimezone(horario_brasil)\n",
    "    timestamp_novo = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    nome_arquivo_novo = f\"{entidade}_{timestamp_novo}.csv\"\n",
    "    caminho_novo = f\"{destino_atual}{nome_arquivo_novo}\"\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(origem + arquivo, caminho_novo)\n",
    "        print(f\"[INFO] Novo arquivo {nome_arquivo_novo} baixado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao baixar {arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46b7258-71ad-41ca-91b6-5cd570738067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir mostra todos os arquivos mais recentes salvos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4accc836-ecf5-49d4-a4d3-e2615dae5b32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "display(dbutils.fs.ls(destino_atual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80f2005-b496-4ed2-98ca-0c38357d842e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "O código a seguir mostra o histórico de todas as versões dos arquivos salvos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5af390-710c-4166-a0a3-0b06a03f823d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "display(dbutils.fs.ls(destino_historico))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1084fdb7-5733-4a33-ab4b-b62fbe9ab502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir mostra um arquivo como exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e112f08-31f2-4287-80ce-d0d2cc3772fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "nome_arquivo = arquivos[0].split('.')[0]\n",
    "arquivos_no_diretorio = dbutils.fs.ls(destino_atual)\n",
    "\n",
    "arquivo_alvo = None\n",
    "\n",
    "for arquivo in arquivos_no_diretorio:\n",
    "    if arquivo.name.startswith(nome_arquivo):\n",
    "        arquivo_alvo = arquivo.path\n",
    "        break\n",
    "\n",
    "if arquivo_alvo:\n",
    "    dataframe_arquivo_exemplo = spark.read.csv(arquivo_alvo, header=True, inferSchema=True)\n",
    "    display(dataframe_arquivo_exemplo)\n",
    "else:\n",
    "    print(\"[INFO] Arquivo não encontrado!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5108017277819344,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 2 - Importação das Bases de Dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
