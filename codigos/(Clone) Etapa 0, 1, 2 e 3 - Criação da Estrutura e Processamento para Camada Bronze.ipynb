{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441ec1a4-c776-45a8-9367-a67090ea9068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a809fe9e-181d-4160-a61f-b52ae5d1e9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 0 - CRIAÇÃO DO DATA LAKEHOUSE E SUAS CAMADAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7325de27-8042-476b-a576-940ff09fe547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Após as etapas de _Levantamento de Requisitos_, _Identificação da Base de Dados_, _Geração do Modelo Conceitual_ e _Verificação pós-mesclagem do Modelo Conceitual com a Base de Dados_. Agora começa a etapa prática do projeto, iniciando pela criação do ambiente onde todos os dados ficarão organizados no Delta Lakehouse. \n",
    "\n",
    "Será utilizada a estrutura:\n",
    "- **Landing Zone** -> Dados totalmente brutos, em seus formatos originais\n",
    "- **Bronze** -> Dados ainda brutos, porém já com formato padrão\n",
    "- **Silver** -> Dados limpos e filtrados\n",
    "- **Gold** -> Dados agrupados, prontos para criação de modelo semânticos e utilização externa.\n",
    "\n",
    "Em resumo, as camadas Landing e Bronze lembram de tudo, a camada Silver entende o passado e a Gold responde o presente.\n",
    "\n",
    "*`Complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ce5606-aca1-4e87-9f01-76f009623049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2cf9f35-6ac8-41f4-9a22-81780ac62051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> _Nessa etapa não há necessidade de importação de bibliotecas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513f5cbd-185a-4936-b50a-902b35d41761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Parte 2 - **Criação do Data Lakehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f4c21e-2fed-4b83-9711-b7006709e8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Complete a variável:`<br>\n",
    "- `nome_datalakehouse --> Nome do Data Lakehouse`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51fc84ef-5bc9-4032-9fe7-700bfea51662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4473ce74-e77e-4760-be3c-a51174491692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui será criado uma **variável** para ajudar a **validar** se o Data Lakehouse que estamos criando foi realmente criado **nesta execução**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6666c416-4134-4e8e-9e4e-6ccb1cfd6042",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0"
    }
   },
   "outputs": [],
   "source": [
    "datalakehouse_foi_criado_agora = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e32cc4b-dd53-4a1a-bb74-a199f3b2c051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui será o código para buscar todos os Data Lakehouses existentes e armazenar o resultado dessa consulta em um Data Frame, assim verificando:\n",
    "  - Se já existir, não acontecerá mais no código para não prejudicar outros projetos que possam já existir.\n",
    "  - Se ainda não existir, cria toda a estrutura esperada, tanto o Data Lakehouse quanto as suas respectivas camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2506c2ac-222a-4d55-9405-a3e1836e30ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0"
    }
   },
   "outputs": [],
   "source": [
    "dataslakeshouses_existentes = spark.sql(\"SHOW CATALOGS\")\n",
    "\n",
    "datalakehouse_foi_criado_agora = not (\n",
    "    dataslakeshouses_existentes\n",
    "    .filter(dataslakeshouses_existentes.catalog == nome_datalakehouse)\n",
    "    .count() > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1d604a-cb68-4391-b770-b733e954d5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui será o código responsável por **criar** o **Data Lakehouse (catálogo)** junto do nome personalizado anteriormente e já colocá-lo em uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ed9466-e6a0-4687-839b-606a46959ce3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0"
    }
   },
   "outputs": [],
   "source": [
    "if datalakehouse_foi_criado_agora:\n",
    "  spark.sql(f\"CREATE CATALOG {nome_datalakehouse}\")\n",
    "  spark.sql(f\"USE CATALOG {nome_datalakehouse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51633943-6f2f-4ee4-af01-6c1f9104f5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 3 - **Criação das Partições do Data LakeHouse** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194c9a09-e274-4bd0-aedc-e7a84b49fe8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Complete a variável:`<br>\n",
    "- `nomes_camadas --> Nome de todas as camadas que deseja na estrutura`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c160b00-ab56-4fc6-bf95-bd2f2a770822",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "nomes_camadas = [\n",
    "    \"audit\", \n",
    "    \"landing_zone\", \n",
    "    \"bronze\", \n",
    "    \"silver\", \n",
    "    \"gold\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7800fe7-c045-4a43-9341-c128c0664004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui será o código para criar as camadas separadamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94631e5c-ff2d-4dff-b45e-b0911ab72e17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0"
    }
   },
   "outputs": [],
   "source": [
    "if datalakehouse_foi_criado_agora:\n",
    "  for camada in nomes_camadas:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {camada}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a477c8-a38e-4575-8bbc-7030c3757dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5b21c7-f574-4fec-9c0d-c19a2dcd52fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8930d0ea-0372-44af-a810-55d73392c6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Código simples para mostrar se deu certo ou não essa etapa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90f5c63-4a7a-44b0-b075-ac26da7f9d0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 0 - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "if datalakehouse_foi_criado_agora:\n",
    "    print(f\"[INFO] O Data LakeHouse '{nome_datalakehouse}' e suas respectivas camadas foram criadas com sucesso!\")\n",
    "else:\n",
    "    print(f\"[INFO] O Data LakeHouse '{nome_datalakehouse}' já existe, escolha outro nome!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b8a412-36f8-4958-b9c8-d3f6fab22f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4b530f-1225-4524-b69f-24e9086db5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 1 - CONFIGURAÇÕES DE GOVERNANÇA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50d85d2-452a-42fc-84a2-408dafca0bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora será feito a configuração do ambiente de logs. Logs são registros de tudo que acontece no ambiente, ou seja, são utilizados para manter um histórico de alterações dos ambientes e dos processos. Além disso, também será criado a tabela para os jobs, responsável por armazenar cada execução, ou seja, conjunto de logs de uma determinada rotina.\n",
    "\n",
    "*`Complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0ca4d0-519a-421e-af8b-f0c865d0eff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e57424-aef9-4ed2-9c31-ec493ff10e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> _Nessa etapa não há necessidade de importação de bibliotecas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfb2173-38db-421e-9255-24d5862f94df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 2 - **Criação das Tabelas da Camada de Logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e020a242-afae-4183-9f88-ac34ed01d1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Complete as variáveis:`<br>\n",
    "- `nome_camada --> o nome da camada destinada para logs e jobs`\n",
    "- `nome_tabelas_logs --> todo os nomes das tabelas onde serão armazenados os logs`\n",
    "- `nome_tabela_jobs --> nome da tabela que armazenará todos os jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef322bda-9270-4d2f-90bf-ef4db8851c1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 1 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "nome_camada = \"audit\"\n",
    "\n",
    "nome_tabelas_logs = [\n",
    "    \"landing_zone_logs\",\n",
    "    \"bronze_logs\",\n",
    "    \"silver_logs\",\n",
    "    \"gold_logs\",\n",
    "    \"analytics_logs\"\n",
    "    ]\n",
    "\n",
    "nome_tabela_jobs = \"jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedea07a-e414-4963-a90e-717a49449a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria as tabelas se elas ainda não existirem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbe3883-7a6d-4732-a9cf-92049b08b69e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 1"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "for tabela in nome_tabelas_logs:\n",
    "  spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{tabela} (\n",
    "    id_log STRING,\n",
    "    id_job STRING,\n",
    "    nome_arquivo STRING,\n",
    "    fonte STRING,\n",
    "    camada STRING,\n",
    "    path_origem STRING,\n",
    "    path_destino STRING,\n",
    "    data_inicio TIMESTAMP,\n",
    "    data_fim TIMESTAMP,\n",
    "    duracao_ms BIGINT,\n",
    "    registros_lidos BIGINT,\n",
    "    registros_gravados BIGINT,\n",
    "    status STRING,\n",
    "    mensagem_erro STRING,\n",
    "    data_execucao DATE\n",
    "    ) USING DELTA \"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_tabela_jobs}  (\n",
    "    id_job STRING,\n",
    "    nome_job STRING,\n",
    "    status STRING,\n",
    "    inicio TIMESTAMP,\n",
    "    fim TIMESTAMP,\n",
    "    duracao_ms BIGINT,\n",
    "    data_execucao DATE\n",
    "    ) USING DELTA \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108ffada-359f-4285-8fdf-cfe2e14f0064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d88d55-7dba-4974-a903-1de95ecdc6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4778cd63-b6b6-46e5-893e-a037850fe34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Código simples para mostrar se deu certo ou não essa etapa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14431d34-b9ca-4cd4-8497-e6250cc13671",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 1 - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "display(spark.sql(f\"SHOW TABLES IN {nome_datalakehouse}.{nome_camada}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2534e85d-8b01-4dd0-8552-2ce5430dc0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f99a62-99d4-4a3f-aaab-8bf4b3f09e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 2 - IMPORTAÇÃO DAS BASES DE DADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca68b933-1c0b-4645-b663-a301e53ac7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora será feito a importação dos dados para a camada especificada do Data LakeHouse que já está pronta.\n",
    "\n",
    "*`Complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90417767-1614-49f9-a8e1-fa69cf93fc33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ed5f7e-f493-4485-b7ae-0614430f849a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c49a258-7c19-4bfb-861e-fb142d1f5c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 2 - **Importação dos Arquivos para o Data Lakehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363bff6a-247a-4c35-b39a-5175a2bc471d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira nas variáveis:` <br>\n",
    "- `nome_datalakehouse --> nome do Data Lakehouse destino`\n",
    "- `nome_camada --> o nome da camada destino dos dados totalmente brutos`\n",
    "- `nome_volume_atual --> nome do volume onde ficaram os dados atualizados`\n",
    "- `nome_volume_historico --> nome do volume onde ficaram o histórico dos dados`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a50e420-429f-4819-bb05-7a15dc6f5d79",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "nome_camada = \"landing_zone\"\n",
    "nome_volume_atual = \"vendas_atual\"\n",
    "nome_volume_historico = \"vendas_historico\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580a8ba4-01a6-472e-905a-f43cc09be62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria os volumes se eles ainda não existirem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba2bc73-94f4-498a-9220-ccfc387a67ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_volume_atual}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_volume_historico}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ee3bd6-aa1b-495e-9df0-6243147fbe2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira nos vetores todos os nomes dos arquivos que deseja importar e a origem deles, seguindo a mesma ordem:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec93462-0a7e-4395-aae8-f704ff26723a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "origem_arquivos = [\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38a122e-493d-473c-95e8-7c8fadd321ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir faz o download do arquivo seguindo a seguinte lógica:\n",
    "- Se o arquivo não existir no sistema ainda, joga ele para a pasta dos arquivos atuais e registra a data no final do seu nome\n",
    "- Se o arquivo já existir, então o mais novo vai para a pasta atual e o anterior igual vai para pasta de histórico, separada granularmente pela data para facilitar consultas futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abf19ad-77ff-4e17-a8d0-a7f3aa8e3b54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2"
    }
   },
   "outputs": [],
   "source": [
    "destino_atual = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume_atual}/\"\n",
    "destino_historico = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume_historico}/\"\n",
    "\n",
    "horario_brasil = datetime.timezone(datetime.timedelta(hours=-3))\n",
    "\n",
    "for arquivo, origem in zip(arquivos, origem_arquivos):\n",
    "    nome_arquivo = arquivo.replace(\".csv\", \"\")\n",
    "    \n",
    "    try:\n",
    "        arquivos_existentes = dbutils.fs.ls(destino_atual)\n",
    "        \n",
    "        for arquivo_existente in arquivos_existentes:\n",
    "            if arquivo_existente.name.startswith(f\"{nome_arquivo}_\") and arquivo_existente.name.endswith(\".csv\"):\n",
    "                \n",
    "                nome_sem_extensao = arquivo_existente.name.replace(\".csv\", \"\")\n",
    "                partes = nome_sem_extensao.split(\"_\")\n",
    "                data_arquivo = partes[-2] \n",
    "                \n",
    "                ano_arquivo = data_arquivo[0:4]\n",
    "                mes_arquivo = data_arquivo[4:6]\n",
    "                dia_arquivo = data_arquivo[6:8]\n",
    "                \n",
    "                pasta_historico_real = f\"{destino_historico}{nome_arquivo}/{ano_arquivo}/{mes_arquivo}/{dia_arquivo}/\"\n",
    "                \n",
    "                dbutils.fs.mkdirs(pasta_historico_real)\n",
    "                \n",
    "                caminho_historico = f\"{pasta_historico_real}{arquivo_existente.name}\"\n",
    "                dbutils.fs.mv(arquivo_existente.path, caminho_historico)\n",
    "                \n",
    "                print(f\"[INFO] Histórico criado e arquivo movido: {caminho_historico}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    data_hora_agora = datetime.datetime.now(datetime.timezone.utc).astimezone(horario_brasil)\n",
    "    data_hora_agora_formatada= data_hora_agora.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    nome_arquivo_novo = f\"{nome_arquivo}_{data_hora_agora_formatada}.csv\"\n",
    "    caminho_novo = f\"{destino_atual}{nome_arquivo_novo}\"\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(origem + arquivo, caminho_novo)\n",
    "        print(f\"[INFO] Novo arquivo {nome_arquivo_novo} baixado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao baixar {arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492842b0-aad7-4441-b95e-24fecca598d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a25ba5-5893-4a31-a4a0-1b0f7b972817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6f9e9c-45fd-468a-804c-028c3c5677b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Código simples para mostrar se deu certo ou não essa etapa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d11136d-2fdc-4920-8b1f-cc46979d9e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Mostrar todos os arquivos mais recentes salvos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf147a5-2a7d-4070-b984-06ca62c9523c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2 - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "display(dbutils.fs.ls(destino_atual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a41692b-ebed-4148-b420-586634f162e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Mostrar o histórico de todas as versões dos arquivos salvos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b3490f-fd4b-49ba-92ba-a9c3c7bd80c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2  - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "display(dbutils.fs.ls(destino_historico))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc6bf0-c25d-432a-bff8-3f8fd4de9d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Mostrar um arquivo como exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001747f0-96ff-487a-8532-61a3a0e3f483",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 2  - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "\n",
    "nome_arquivo = arquivos[0].split('.')[0]\n",
    "arquivos_no_diretorio = dbutils.fs.ls(destino_atual)\n",
    "\n",
    "arquivo_alvo = None\n",
    "\n",
    "for arquivo in arquivos_no_diretorio:\n",
    "    if arquivo.name.startswith(nome_arquivo):\n",
    "        arquivo_alvo = arquivo.path\n",
    "        break\n",
    "\n",
    "if arquivo_alvo:\n",
    "    dataframe_arquivo_exemplo = spark.read.csv(arquivo_alvo, header=True, inferSchema=True)\n",
    "    display(dataframe_arquivo_exemplo)\n",
    "else:\n",
    "    print(\"[INFO] Arquivo não encontrado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4259e8-d308-406c-bfa6-5b554b235a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22302f13-1e83-41a2-a402-e79880b12c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 3 - PROCESSAMENTO PARA CAMADA BRONZE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7115dd1b-289a-4a0e-9a14-8a89c3ecf0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada bronze, colocando em um formato padrão mas sem mais nenhum refinamento, ainda mantendo os dados brutos.\n",
    "\n",
    "*`Complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6326f97-ed60-4c12-aaad-d6f38956e5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d44a11-3480-48e1-a052-ffeb453666d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda4cbfb-dc88-4794-a813-c5a5defa560d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2ac13e-33ca-41be-a1ec-d575f21a5b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5525d53-3e0c-4c97-aedc-ce08a2d82fd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load Data Bronze\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1c6867-f889-4e3f-a09c-85da86868eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 3 - **Definindo Origens, Arquivos e Destinos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ebbe40-d2b2-40a5-b5df-7b2807af1b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira nas variáveis:` <br>\n",
    "- `nome_camada_origem --> nome da camada de origem dos dados`\n",
    "- `nome_volume_origem --> nome do volume de origem dos dados dentro da camada` \n",
    "- `nome_camada_bronze --> nome da camada de destino dos dados`\n",
    "- `nome_volume_atual --> nome do volume de destino dos dados mais atualizados dentro da camada`\n",
    "- `nome_volume_historico --> nome do volume de destino dos dados antigos dentro da camada` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef936158-ae19-4684-9802-2e0e803f9982",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "nome_camada_origem = \"landing_zone\"\n",
    "nome_volume_origem = \"vendas_atual\"\n",
    "\n",
    "nome_camada_bronze = \"bronze\"\n",
    "nome_volume_atual = \"vendas_atual\" \n",
    "nome_volume_historico = \"vendas_historico\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae6b10d-bdb1-49a9-baef-f2cdd03db8e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e84a8ff-7d32-4f95-b936-0cff29a9e7cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "\n",
    "destino_atual = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_atual}/\"\n",
    "destino_historico = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_historico}/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d5e411-95a4-41ea-8535-49e91535f764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria a estrutura dos volumes de destino, caso eles ainda não existam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c0a433-89c1-4f5a-aef6-dafc07f6b593",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_atual}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_historico}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c197b7e-e783-4ccc-bd40-e569528368a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira no vetor todos os arquivos que deseja fazer esse processamento para camada bronze:` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e87453b-0e99-4d40-99e0-50ffb6b69013",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3 - Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "arquivos_landing_zone = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e13d6c-4bc7-49b8-84bf-91bb2b97d8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 4 - **Leitura dos Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c1d983-5b7b-452a-9eee-552735a306d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para que seja possível manipular os dados, antes precisamos ler todos eles já com uma estrutura pré-definida.\n",
    "\n",
    "`Para cada estrutura, é necessário elaborar manualmente como deve ser interpretada`\n",
    "\n",
    "O código a seguir cria essas estruturas para cada arquivo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de536ba-f75f-4bd8-bf60-8581ed0c24f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"categoria_produto.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"categoria_id\", LongType()),\n",
    "            StructField(\"categoria_nome\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"cliente.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"cliente_id\", LongType()),\n",
    "            StructField(\"nome_cliente\", StringType()),\n",
    "            StructField(\"estado\", StringType()),\n",
    "            StructField(\"cidade\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"data.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"data_id\", LongType()),\n",
    "            StructField(\"data\", DateType()),\n",
    "            StructField(\"ano\", IntegerType()),\n",
    "            StructField(\"mes\", IntegerType()),\n",
    "            StructField(\"dia\", IntegerType()),\n",
    "            StructField(\"dia_semana\", StringType()),\n",
    "            StructField(\"final_de_semana\", ByteType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"localidade.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"localidade_id\", LongType()),\n",
    "            StructField(\"estado\", StringType()),\n",
    "            StructField(\"cidade\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"produto.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"produto_id\", LongType()),\n",
    "            StructField(\"preco_lista\", DoubleType()),\n",
    "            StructField(\"categoria_nome\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"vendas\": StructType(\n",
    "        [\n",
    "            StructField(\"venda_id\", LongType()),\n",
    "            StructField(\"cliente_id\", LongType()),\n",
    "            StructField(\"produto_id\", LongType()),\n",
    "            StructField(\"data_id\", LongType()),\n",
    "            StructField(\"categoria_id\", LongType()),\n",
    "            StructField(\"localidade_id\", LongType()),\n",
    "            StructField(\"quantidade\", LongType()),\n",
    "            StructField(\"preco_lista\", DoubleType()),\n",
    "            StructField(\"valor_total\", DoubleType()),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88331da-be70-43b8-ba59-aabc4ba95b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir faz a leitura dos dados em cada estrutura, adicionando a coluna nome do arquivo de origem e a data que foi feito. armazenando o resultado desse processo em um vetor de datas frames para que seja possível manipular esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb82f18-f0b3-4f4e-8811-5e300d758a23",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "try:\n",
    "    arquivos_existentes = dbutils.fs.ls(origem_dados)\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao acessar volume: {e}\")\n",
    "    arquivos_existentes = []\n",
    "\n",
    "for arquivo_origem in arquivos_landing_zone:\n",
    "    try:\n",
    "        nome_arquivo = arquivo_origem.replace(\".csv\", \"\")\n",
    "        arquivo_encontrado = None\n",
    "\n",
    "        for arquivo_existente in arquivos_existentes:\n",
    "            if arquivo_existente.name.startswith(f\"{nome_arquivo}_\") and arquivo_existente.name.endswith(\".csv\"):\n",
    "                arquivo_encontrado = arquivo_existente\n",
    "                break\n",
    "\n",
    "        if not arquivo_encontrado:\n",
    "            print(f\"[INFO] Arquivo para '{nome_arquivo}' não encontrado na Landing.\")\n",
    "            continue\n",
    "\n",
    "        schema = schemas[\"vendas\"] if nome_arquivo.startswith(\"vendas\") else schemas[arquivo_origem]\n",
    "\n",
    "        dataframe = (\n",
    "            spark.read.option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .csv(arquivo_encontrado.path)\n",
    "            .withColumn(\"_source_file\", lit(arquivo_encontrado.name))\n",
    "            .withColumn(\"_ingestion_date\", current_timestamp())\n",
    "        )\n",
    "\n",
    "        dataframes[nome_arquivo] = dataframe\n",
    "        print(f\"[INFO] Leitura de {arquivo_encontrado.name} realizada.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao processar {arquivo_origem}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ff719a-8b90-4bfd-8a42-17e3cac8a9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 5 - **Salvando os Dados no Formato e Local Corretos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247e52e5-651f-4ff2-955f-8da0bac2ab7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por fim, precisamos criar o parquet com os dados que importamos e com a estrutura que construimos anteriormente e mover para camada bronze.\n",
    "\n",
    "O código a seguir faz essa compressão e envio para o caminho certo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d31824-f845-40d9-abb0-fb5e60f695eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "data_hora_agora = datetime.datetime.now(datetime.timezone.utc).astimezone(horario_brasil)\n",
    "ano = data_hora_agora.strftime(\"%Y\")\n",
    "mes = data_hora_agora.strftime(\"%m\")\n",
    "dia = data_hora_agora.strftime(\"%d\")\n",
    "data_hora_agora_formatada = data_hora_agora.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def salvar_bronze(dataframe_pronto, nome_tabela):\n",
    "    caminho_atual = f\"{destino_atual}/{nome_tabela}\"\n",
    "    \n",
    "    pasta_historico = f\"{destino_historico}/{nome_tabela}/{ano}/{mes}/{dia}/\"\n",
    "    caminho_historico = f\"{pasta_historico}/{nome_tabela}_{data_hora_agora_formatada}\"\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.ls(caminho_atual)\n",
    "        dbutils.fs.mkdirs(pasta_historico) \n",
    "        dbutils.fs.mv(caminho_atual, caminho_historico, recurse=True)\n",
    "        print(f\"[INFO] Tabela anterior de {nome_tabela} arquivada em: {pasta_historico}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    dataframe_pronto.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(caminho_atual)\n",
    "    print(f\"[INFO] {nome_tabela} atualizada com sucesso na Bronze Atual.\")\n",
    "\n",
    "dimensoes = {\n",
    "    \"categoria_produto\": \"bronze_dim_categoria_produto\",\n",
    "    \"cliente\": \"bronze_dim_cliente\",\n",
    "    \"data\": \"bronze_dim_data\",\n",
    "    \"localidade\": \"bronze_dim_localidade\",\n",
    "    \"produto\": \"bronze_dim_produto\"\n",
    "}\n",
    "\n",
    "for nome_arquivo_bruto, nome_arquivo_formatado in dimensoes.items():\n",
    "    if nome_arquivo_bruto in dataframes:\n",
    "        salvar_bronze(dataframes[nome_arquivo_bruto], nome_arquivo_formatado)\n",
    "\n",
    "vendas_dataframes = [dataframe_vendas for nome, dataframe_vendas in dataframes.items() if nome.startswith(\"vendas_part\")]\n",
    "\n",
    "if vendas_dataframes:\n",
    "    dataframe_vendas_pronto = vendas_dataframes[0]\n",
    "    for dataframes_outras_partes in vendas_dataframes[1:]:\n",
    "        dataframe_vendas_pronto = dataframe_vendas_pronto.unionAll(dataframes_outras_partes)\n",
    "    \n",
    "    salvar_bronze(dataframe_vendas_pronto, \"bronze_fato_vendas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4656438-c43e-40a0-83bb-4b6abf22fec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> O modo de escrita define como os dados serão gravados no destino:\n",
    "- ***overwrite***: remove os dados existentes no caminho/partições e grava tudo novamente\n",
    "- ***append***: adiciona novos dados às partições existentes, sem apagar o que já existe \n",
    "\n",
    "> Em pipelines produtivos, 'append' é o mais comum para cargas incrementais. 'overwrite' costuma ser usado apenas em reprocessamentos completos ou ambientes de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7a00a3-f1c7-47e0-9bea-532d6ba9af75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 6 - **Limpeza de Cache e Outros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06fb395-ea80-449a-a4e3-9f325cc19fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir libera memória de objetos não mais utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea2879a-4505-496a-9033-57fccff94269",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3"
    }
   },
   "outputs": [],
   "source": [
    "for dataframe in dataframes.values():\n",
    "    del dataframe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e491e0d-dc1f-43d5-8746-cce307364508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ae11e-50a4-4e74-b141-cc5d22aa0253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf759508-beb0-4a75-b073-763dad8b8cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Código simples para mostrar se deu certo ou não essa etapa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c181953-d2be-4716-81a1-f5ff49c3bdb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETAPA 3 - Resultados"
    }
   },
   "outputs": [],
   "source": [
    "# %skip\n",
    "dataframe_bronze = spark.read.format(\"delta\").load(destino_atual + \"/bronze_dim_categoria_produto\")\n",
    "\n",
    "display(dataframe_bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Etapa 0, 1, 2 e 3 - Criação da Estrutura e Processamento para Camada Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
