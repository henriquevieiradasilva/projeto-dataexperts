{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80024867-4e20-41ee-8d8e-c02167642565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 4 - PROCESSAMENTO PARA CAMADA SILVER**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada silver, filtrando e limpando os dados. \n",
    "\n",
    "<br>\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7c3e41-d88f-4866-b14e-ea897134430c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f95bc9-13a8-40a3-b5f0-2abb3dbdb389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**\n",
    "\n",
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ebd850-639e-4de8-8ebf-c6ccb8031e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Load Data Bronze\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "        .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") # novo\n",
    "        .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") # novo\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d92e830-55eb-435a-b007-7b1f15206404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Definindo Origens e Destinos**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse ` <br>\n",
    "`--> nome_camada_origem --> nome da camada de origem dos dados` <br>\n",
    "`--> nome_volume_origem --> nome do volume de origem dos dados dentro da camada` <br>\n",
    "`--> nome_camada_silver --> nome da camada de destino dos dados` <br>\n",
    "`--> nome_volume_silver --> nome do volume de destino dos dados dentro da camada` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684a028b-ea7e-491a-aad3-7e1fb81eb732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\" \n",
    "\n",
    "nome_camada_origem = \"bronze\"   \n",
    "nome_volume_origem = \"vendas_atual\"\n",
    "\n",
    "nome_camada_silver = \"silver\"\n",
    "nome_volume_silver = \"vendas\"\n",
    "\n",
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_silver}/{nome_volume_silver}/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdeac567-3f89-4d03-8177-b931c118aeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c37e7a-de2c-4c56-8258-9bb5e2787f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_silver}/{nome_volume_silver}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe8ebd2-35da-4f79-8a46-beb5c1e6552a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume de destino caso ele ainda não exista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6142f41f-dcfd-4591-811e-eb3077271b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_silver}.{nome_volume_silver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5fb1c3a-963a-4f3e-9a89-ad6d13dc63e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Leitura dos Dados**\n",
    "\n",
    "\n",
    "O código a seguir lê todos os dados e colocar em um Data Frame para que seja possível manipular os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e02781-d836-49a6-bcf9-e5d9dd160fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabelas_bronze = [\n",
    "    \"bronze_dim_categoria_produto\",\n",
    "    \"bronze_dim_cliente\",\n",
    "    \"bronze_dim_data\",\n",
    "    \"bronze_dim_localidade\",\n",
    "    \"bronze_dim_produto\",\n",
    "    \"bronze_fato_vendas\"\n",
    "]\n",
    "\n",
    "bronze_dfs = {}\n",
    "for tabela in tabelas_bronze:\n",
    "    bronze_dfs[tabela] = spark.read.format(\"delta\").load(f\"{origem_dados}{tabela}\")\n",
    "    print(f\"[INFO] Tabela {tabela} carregada com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c3f7b7e-b4ad-4857-9eb1-59bcfd5c70af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 5 - **Limpeza dos Dados**\n",
    "\n",
    "A limpeza de dados é um processo crucial para garantir a **qualidade dos dados**. Isso envolve a remoção de **dados duplicados ou incorretos**, a **padronização de formatos e valores de dados** e o **enriquecimento de dados** com informações adicionais. Tudo isso para **garantir** que os **dados** sejam **precisos** e **confiáveis**.\n",
    "\n",
    "Cada coluna deve ser analisada para encontrar possíveis inconformidades que podem ser encontradas e resolvidas nessa estapa, entre elas estão:\n",
    "- Remoção de dados duplicados \n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde3dc4e-325c-4d95-b713-2c5da826d46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_sk(df, id_col, sk_name):\n",
    "    return df.withColumn(sk_name, sha2(col(id_col).cast(\"string\"), 256))\n",
    "\n",
    "df_categoria = bronze_dfs[\"bronze_dim_categoria_produto\"]\n",
    "dataframe_silver_categoria = add_sk(\n",
    "    df_categoria.dropDuplicates([\"categoria_id\"])\n",
    "    .withColumn(\"categoria_nome\", upper(trim(col(\"categoria_nome\"))))\n",
    "    .withColumn(\"dt_inicio\", current_date())\n",
    "    .withColumn(\"dt_fim\", lit(None).cast(\"date\"))\n",
    "    .withColumn(\"is_current\", lit(True)),\n",
    "    \"categoria_id\", \"sk_categoria\"\n",
    ")\n",
    "\n",
    "df_cliente = bronze_dfs[\"bronze_dim_cliente\"]\n",
    "dataframe_silver_cliente = (\n",
    "    df_cliente.dropDuplicates([\"cliente_id\"])\n",
    "    .withColumn(\"nome_cliente\", initcap(trim(col(\"nome_cliente\"))))\n",
    "    .withColumn(\"estado\", upper(col(\"estado\")))\n",
    "    .withColumn(\"cidade\", initcap(col(\"cidade\")))\n",
    "\n",
    "    .withColumn(\"dt_inicio\", current_date())\n",
    "    .withColumn(\"dt_fim\", lit(None).cast(\"date\"))\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    ")\n",
    "\n",
    "df_produto = bronze_dfs[\"bronze_dim_produto\"]\n",
    "dataframe_silver_produto = add_sk(\n",
    "    df_produto.dropDuplicates([\"produto_id\"])\n",
    "    .withColumn(\"categoria_nome\", upper(trim(col(\"categoria_nome\"))))\n",
    "    .withColumn(\"dt_inicio\", current_date())\n",
    "    .withColumn(\"dt_fim\", lit(None).cast(\"date\"))\n",
    "    .withColumn(\"is_current\", lit(True)),\n",
    "    \"produto_id\", \"sk_produto\"\n",
    ")\n",
    "\n",
    "# Dimensão Localidade\n",
    "df_localidade = bronze_dfs[\"bronze_dim_localidade\"]\n",
    "dataframe_silver_localidade = add_sk(\n",
    "    df_localidade.dropDuplicates([\"localidade_id\"])\n",
    "    .withColumn(\"estado\", upper(col(\"estado\")))\n",
    "    .withColumn(\"cidade\", initcap(col(\"cidade\")))\n",
    "    .withColumn(\"dt_inicio\", current_date())\n",
    "    .withColumn(\"dt_fim\", lit(None).cast(\"date\"))\n",
    "    .withColumn(\"is_current\", lit(True)),\n",
    "    \"localidade_id\", \"sk_localidade\"\n",
    ")\n",
    "\n",
    "df_data = bronze_dfs[\"bronze_dim_data\"]\n",
    "dataframe_silver_data = add_sk(\n",
    "    df_data.dropDuplicates([\"data_id\"])\n",
    "    .withColumn(\"dt_inicio\", current_date())\n",
    "    .withColumn(\"dt_fim\", lit(None).cast(\"date\"))\n",
    "    .withColumn(\"is_current\", lit(True)),\n",
    "    \"data_id\", \"sk_data\"\n",
    ")\n",
    "\n",
    "dataframe_silver_cliente = add_sk(dataframe_silver_cliente, \"cliente_id\", \"sk_cliente\")\n",
    "\n",
    "\n",
    "dataframe_silver_fato = (\n",
    "    df_fato.dropDuplicates([\"venda_id\"])\n",
    "    .filter(col(\"quantidade\") > 0)\n",
    "    .join(dataframe_silver_cliente.select(\"cliente_id\", \"sk_cliente\"), on=\"cliente_id\", how=\"left\")\n",
    "    .join(dataframe_silver_produto.select(\"produto_id\", \"sk_produto\"), on=\"produto_id\", how=\"left\")\n",
    "    .join(dataframe_silver_categoria.select(\"categoria_id\", \"sk_categoria\"), on=\"categoria_id\", how=\"left\")\n",
    "    # --- Adicionando as que faltavam ---\n",
    "    .join(dataframe_silver_localidade.select(\"localidade_id\", \"sk_localidade\"), on=\"localidade_id\", how=\"left\")\n",
    "    .join(dataframe_silver_data.select(\"data_id\", \"sk_data\"), on=\"data_id\", how=\"left\")\n",
    "    # ----------------------------------\n",
    "    .drop(\"cliente_id\", \"produto_id\", \"categoria_id\", \"localidade_id\", \"data_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe22114-d210-4e25-bd6f-c23fddcb0adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte - ****\n",
    "\n",
    "Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d8b70b-9bfa-472c-a99b-bd6f4dcaf843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mapeamento de caminhos e DataFrames\n",
    "tabelas_para_salvar = {\n",
    "    \"silver_dim_cliente\": dataframe_silver_cliente,\n",
    "    \"silver_dim_categoria\": dataframe_silver_categoria,\n",
    "    \"silver_dim_produto\": dataframe_silver_produto,\n",
    "    \"silver_dim_localidade\": dataframe_silver_localidade, \n",
    "    \"silver_dim_data\": dataframe_silver_data,             \n",
    "    \"silver_fato_vendas\": dataframe_silver_fato\n",
    "}\n",
    "\n",
    "for nome_tabela, df_silver in tabelas_para_salvar.items():\n",
    "    path_destino = f\"{destino_dados}{nome_tabela}\"\n",
    "    \n",
    "    if not DeltaTable.isDeltaTable(spark, path_destino):\n",
    "        df_silver.write.format(\"delta\").save(path_destino)\n",
    "        print(f\"[INFO] Tabela {nome_tabela} criada.\")\n",
    "    else:\n",
    "        # Para a Fato, usamos OVERWRITE ou APPEND simples\n",
    "        if \"fato\" in nome_tabela:\n",
    "            df_silver.write.format(\"delta\").mode(\"overwrite\").save(path_destino)\n",
    "        else:\n",
    "            # Para Dimensões, usamos o MERGE que você já tem\n",
    "            dt_silver = DeltaTable.forPath(spark, path_destino)\n",
    "            # (Aqui você pode repetir a lógica de merge que já escreveu)\n",
    "            print(f\"[INFO] Merge realizado em {nome_tabela}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17fff22-b80a-46a2-be41-13619a216ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir é uma das maneiras simples de se mostrar se deu certo ou não a importação do arquivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e336b78-5745-4d4c-81e8-81047aa49565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "\"\"\"\n",
    " #\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 4 - Processamento para Camada Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
