{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfecaa2d-0e0b-4caa-bd24-7d2d4844f5c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# =====================================================\n",
    "# 1. SESS√ÉO SPARK\n",
    "# =====================================================\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# =====================================================\n",
    "# 2. CONFIGURA√á√ïES\n",
    "# =====================================================\n",
    "\n",
    "# Origem (somente leitura)\n",
    "catalogo_origem = \"dataexperts\"\n",
    "schema_gold = \"gold\"\n",
    "\n",
    "# Destino (novo, isolado)\n",
    "catalogo_export = \"dataexports\"\n",
    "schema_export = \"export\"\n",
    "volume_export = \"export_csv\"\n",
    "\n",
    "# Caminho f√≠sico final\n",
    "base_destino = f\"/Volumes/{catalogo_export}/{schema_export}/{volume_export}\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. CRIA√á√ÉO DO CAT√ÅLOGO DE EXPORTA√á√ÉO\n",
    "# =====================================================\n",
    "spark.sql(f\"\"\"\n",
    "CREATE CATALOG IF NOT EXISTS {catalogo_export}\n",
    "\"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. CRIA√á√ÉO DO SCHEMA DE EXPORTA√á√ÉO\n",
    "# =====================================================\n",
    "spark.sql(f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS {catalogo_export}.{schema_export}\n",
    "\"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. CRIA√á√ÉO DO VOLUME PARA CSV\n",
    "# =====================================================\n",
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS\n",
    "{catalogo_export}.{schema_export}.{volume_export}\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìÅ Cat√°logo, schema e volume de exporta√ß√£o prontos.\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. LISTAR TODAS AS TABELAS DA GOLD\n",
    "# =====================================================\n",
    "tabelas_gold = (\n",
    "    spark.sql(f\"SHOW TABLES IN {catalogo_origem}.{schema_gold}\")\n",
    "         .select(\"tableName\")\n",
    "         .collect()\n",
    ")\n",
    "\n",
    "if not tabelas_gold:\n",
    "    raise Exception(\"‚ùå Nenhuma tabela encontrada na camada GOLD.\")\n",
    "\n",
    "print(\"üì¶ Tabelas encontradas na GOLD:\")\n",
    "for t in tabelas_gold:\n",
    "    print(f\" - {t.tableName}\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. EXPORTAR CADA TABELA PARA CSV\n",
    "# =====================================================\n",
    "for t in tabelas_gold:\n",
    "    nome_tabela = t.tableName\n",
    "\n",
    "    print(f\"\\nüöÄ Exportando tabela: {nome_tabela}\")\n",
    "\n",
    "    # leitura da tabela gold\n",
    "    df = spark.table(f\"{catalogo_origem}.{schema_gold}.{nome_tabela}\")\n",
    "\n",
    "    # caminhos\n",
    "    pasta_tabela = f\"{base_destino}/{nome_tabela}\"\n",
    "    pasta_tmp = f\"{pasta_tabela}/_tmp\"\n",
    "\n",
    "    # escrita tempor√°ria (Spark sempre cria pasta)\n",
    "    (\n",
    "        df.coalesce(1)\n",
    "          .write\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .csv(pasta_tmp)\n",
    "    )\n",
    "\n",
    "    # localizar o CSV gerado\n",
    "    arquivos = dbutils.fs.ls(pasta_tmp)\n",
    "    arquivo_csv = [f.path for f in arquivos if f.name.endswith(\".csv\")][0]\n",
    "\n",
    "    # mover para nome final\n",
    "    caminho_final = f\"{pasta_tabela}/{nome_tabela}.csv\"\n",
    "    dbutils.fs.mkdirs(pasta_tabela)\n",
    "    dbutils.fs.mv(arquivo_csv, caminho_final)\n",
    "\n",
    "    # limpar pasta tempor√°ria\n",
    "    dbutils.fs.rm(pasta_tmp, recurse=True)\n",
    "\n",
    "    print(f\"‚úÖ {nome_tabela}.csv exportado com sucesso.\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. FINALIZA√á√ÉO\n",
    "# =====================================================\n",
    "print(\"\\nüéâ EXPORTA√á√ÉO FINALIZADA COM SUCESSO!\")\n",
    "print(f\"üìÇ Arquivos dispon√≠veis em: {base_destino}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567bd31c-84fc-4040-985e-fdea3309e404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa EXTRA 3 - Baixar Resultado da Gold como Csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
