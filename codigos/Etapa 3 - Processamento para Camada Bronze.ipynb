{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672cd9a8-0a64-4900-9601-f8f68cc25a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 3 - PROCESSAMENTO PARA CAMADA BRONZE**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada bronze, colocando em um formato padrão mas sem mais nenhum refinamento, ainda mantendo os dados brutos.\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5189e80d-cbdc-4c12-b5af-5d412e62fe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import * # Simplifica a importação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b38c156-a91f-4ec5-936b-324b57f4df20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**\n",
    "\n",
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b93818-9cf5-4998-9af0-b1190f26c82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load Data Bronze\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4ecd4f-680a-4c37-afac-19f694c45665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Definindo Origens, Arquivos e Destinos**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse ` <br>\n",
    "\n",
    "`--> nome_camada_origem --> nome da camada de origem dos dados` <br>\n",
    "`--> nome_volume_origem --> nome do volume de origem dos dados dentro da camada` <br>\n",
    "\n",
    "`--> nome_camada_bronze --> nome da camada de destino dos dados` <br>\n",
    "`--> nome_volume_atual --> nome do volume de destino dos dados mais atualizados dentro da camada` <br>\n",
    "`--> nome_volume_historico --> nome do volume de destino dos dados antigos dentro da camada` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28a2622-8e18-4bb2-999c-6e5b18c93766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "\n",
    "nome_camada_origem = \"landing_zone\"\n",
    "nome_volume_origem = \"vendas_atual\"\n",
    "\n",
    "nome_camada_bronze = \"bronze\"\n",
    "nome_volume_atual = \"vendas_atual\" \n",
    "nome_volume_historico = \"vendas_historico\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672b818d-7a41-4de0-b660-4e2b3c31919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6524bea1-6541-4b2a-86f2-fdd233857669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "\n",
    "destino_atual = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_atual}/\"\n",
    "destino_historico = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_historico}/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd534185-2e50-4d4e-9765-f9eab33b9742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume de destino caso ele ainda não exista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad938f9-7d50-4430-b4a0-36e419c4161c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_atual}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_historico}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f46b91-bb26-4ea6-b6b0-e32703cbdaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira no vetor todos os arquivos que deseja fazer esse processamento para camada bronze:` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d975b5e-91f2-432b-8439-df7ef09481e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7add9c-0e8a-4eaf-bf9f-454df4bd5177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Leitura dos Dados**\n",
    "\n",
    "Para que seja possível manipular os dados, antes precisamos ler todos eles já com uma estrutura pré-definida.\n",
    "\n",
    "`Para cada estrutura, é necessário elaborar manualmente como deve ser interpretada`\n",
    "\n",
    "O código a seguir cria essas estruturas para cada arquivo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76db3fa0-8b17-4855-b1ad-2181380db5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"categoria_produto.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"categoria_id\", LongType()),\n",
    "            StructField(\"categoria_nome\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"cliente.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"cliente_id\", LongType()),\n",
    "            StructField(\"nome_cliente\", StringType()),\n",
    "            StructField(\"estado\", StringType()),\n",
    "            StructField(\"cidade\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"data.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"data_id\", LongType()),\n",
    "            StructField(\"data\", DateType()),\n",
    "            StructField(\"ano\", IntegerType()),\n",
    "            StructField(\"mes\", IntegerType()),\n",
    "            StructField(\"dia\", IntegerType()),\n",
    "            StructField(\"dia_semana\", StringType()),\n",
    "            StructField(\"final_de_semana\", BooleanType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"localidade.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"localidade_id\", LongType()),\n",
    "            StructField(\"estado\", StringType()),\n",
    "            StructField(\"cidade\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"produto.csv\": StructType(\n",
    "        [\n",
    "            StructField(\"produto_id\", LongType()),\n",
    "            StructField(\"preco_lista\", DoubleType()),\n",
    "            StructField(\"categoria_nome\", StringType()),\n",
    "        ]\n",
    "    ),\n",
    "    \"vendas\": StructType(\n",
    "        [\n",
    "            StructField(\"venda_id\", LongType()),\n",
    "            StructField(\"cliente_id\", LongType()),\n",
    "            StructField(\"produto_id\", LongType()),\n",
    "            StructField(\"data_id\", LongType()),\n",
    "            StructField(\"categoria_id\", LongType()),\n",
    "            StructField(\"localidade_id\", LongType()),\n",
    "            StructField(\"quantidade\", LongType()),\n",
    "            StructField(\"preco_lista\", DoubleType()),\n",
    "            StructField(\"valor_total\", DoubleType()),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f00d122-51be-497c-a94f-81b1a955dd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir faz a leitura dos dados em cada estrutura, adicionando a coluna nome do arquivo durante a leitura, e armarzenando o resultado desse processo em um vetor de datas frames para que seja possível manipular esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad0d4aa-c1f4-4caf-ae71-0abb500a9d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframes_dict = {}\n",
    "\n",
    "try:\n",
    "    # Substituindo os.listdir por dbutils nativo\n",
    "    arquivos_no_volume = dbutils.fs.ls(origem_dados)\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao acessar volume: {e}\")\n",
    "    arquivos_no_volume = []\n",
    "\n",
    "for arquivo_base in arquivos:\n",
    "    try:\n",
    "        entidade = arquivo_base.replace(\".csv\", \"\")\n",
    "        arquivo_encontrado = None\n",
    "\n",
    "        # Busca o arquivo que começa com a entidade no volume atual\n",
    "        for f in arquivos_no_volume:\n",
    "            if f.name.startswith(f\"{entidade}_\") and f.name.endswith(\".csv\"):\n",
    "                arquivo_encontrado = f\n",
    "                break\n",
    "\n",
    "        if not arquivo_encontrado:\n",
    "            print(f\"[INFO] Arquivo para '{entidade}' não encontrado na Landing.\")\n",
    "            continue\n",
    "\n",
    "        # Seleciona o schema correto\n",
    "        schema = schemas[\"vendas\"] if entidade.startswith(\"vendas\") else schemas[arquivo_base]\n",
    "\n",
    "        # Leitura com metadados de auditoria\n",
    "        df = (\n",
    "            spark.read.option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .csv(arquivo_encontrado.path)\n",
    "            .withColumn(\"_source_file\", lit(arquivo_encontrado.name))\n",
    "            .withColumn(\"_ingestion_date\", current_timestamp())\n",
    "        )\n",
    "\n",
    "        dataframes_dict[entidade] = df\n",
    "        print(f\"[INFO] Leitura de {arquivo_encontrado.name} realizada.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao processar {arquivo_base}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9de94b-2481-4482-ba2c-18069d1aadde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 5 - **Salvando os Dados no Formato e Local Corretos**\n",
    "\n",
    "Por fim, precisamos criar o parquet com os dados que importamos e com a estrutura que construimos anteriormente e mover para camada bronze.\n",
    "\n",
    "O código a seguir faz essa compressão e envio para o caminho certo (já particionando em ano e mês para ser mais otimizado quando precisarmos utilizar esses dados granularmente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9b5d26-66aa-49e1-953b-d2f13e32597b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de tempo para o histórico físico da Bronze\n",
    "horario_brasil = datetime.timezone(datetime.timedelta(hours=-3))\n",
    "now = datetime.datetime.now(datetime.timezone.utc).astimezone(horario_brasil)\n",
    "ano, mes, dia = now.strftime(\"%Y\"), now.strftime(\"%m\"), now.strftime(\"%d\")\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def salvar_com_rotacao(df, nome_tabela):\n",
    "    caminho_atual = f\"{destino_atual}/{nome_tabela}\"\n",
    "    # Definimos o caminho, mas NÃO criamos a pasta ainda\n",
    "    pasta_historico = f\"{destino_historico}/{nome_tabela}/{ano}/{mes}/{dia}/\"\n",
    "    caminho_historico = f\"{pasta_historico}/{nome_tabela}_{timestamp}\"\n",
    "\n",
    "    # 1. Tenta mover o antigo para o histórico SOMENTE se ele existir\n",
    "    try:\n",
    "        dbutils.fs.ls(caminho_atual) # Verifica se a pasta atual existe\n",
    "        \n",
    "        dbutils.fs.mkdirs(pasta_historico) # Cria a subpasta de data sob demanda\n",
    "        dbutils.fs.mv(caminho_atual, caminho_historico, recurse=True)\n",
    "        print(f\"[INFO] Tabela anterior de {nome_tabela} arquivada em: {pasta_historico}\")\n",
    "    except:\n",
    "        # Se cair aqui, é porque a tabela atual ainda não existe (primeira carga)\n",
    "        pass\n",
    "\n",
    "    # 2. Salva a nova versão (Sobrescreve a pasta atual vazia ou cria nova)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(caminho_atual)\n",
    "    print(f\"[INFO] {nome_tabela} atualizada com sucesso na Bronze Atual.\")\n",
    "\n",
    "# --- Execução das Dimensões ---\n",
    "dimensoes_mapeamento = {\n",
    "    \"categoria_produto\": \"bronze_dim_categoria_produto\",\n",
    "    \"cliente\": \"bronze_dim_cliente\",\n",
    "    \"data\": \"bronze_dim_data\",\n",
    "    \"localidade\": \"bronze_dim_localidade\",\n",
    "    \"produto\": \"bronze_dim_produto\"\n",
    "}\n",
    "\n",
    "for entidade, nome_tabela in dimensoes_mapeamento.items():\n",
    "    if entidade in dataframes_dict:\n",
    "        salvar_com_rotacao(dataframes_dict[entidade], nome_tabela)\n",
    "\n",
    "# --- Execução da Fato Vendas (Union All) ---\n",
    "vendas_dfs = [df for nome, df in dataframes_dict.items() if nome.startswith(\"vendas_part\")]\n",
    "\n",
    "if vendas_dfs:\n",
    "    df_fato_vendas = vendas_dfs[0]\n",
    "    for extra_df in vendas_dfs[1:]:\n",
    "        df_fato_vendas = df_fato_vendas.unionAll(extra_df)\n",
    "    \n",
    "    salvar_com_rotacao(df_fato_vendas, \"bronze_fato_vendas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36f2d92-973d-42d5-a423-e67ab7d9dac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> O modo de escrita define como os dados serão gravados no destino:\n",
    "- ***overwrite***: remove os dados existentes no caminho/partições e grava tudo novamente\n",
    "- ***append***: adiciona novos dados às partições existentes, sem apagar o que já existe \n",
    "\n",
    "> Em pipelines produtivos, 'append' é o mais comum para cargas incrementais. 'overwrite' costuma ser usado apenas em reprocessamentos completos ou ambientes de teste\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 6 - **Limpeza de Cache e Outros**\n",
    "\n",
    "O código a seguir exclui todos os Data Frames que podem estar em cache no algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c470311-85ab-4434-ae15-5e122826c94e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No Databricks Free Edition (Serverless), o gerenciamento de cache e memória é feito automaticamente pela plataforma. Por esse motivo, alguns comandos tradicionais do Spark não são suportados, como:\n",
    "\n",
    "- dataframe.unpersist()\n",
    "- spark.catalog.clearCache()\n",
    "\n",
    "A alternativa é remover referências aos DataFrames manualmente e forçar a coleta de lixo do Python quando necessário.\n",
    "\"\"\"\n",
    "\n",
    "for df in dataframes_dict.values():\n",
    "    del df\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799a966e-528a-4009-b5b4-01067af1acfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir monta um novo Data Frame com os dados exemplos diretamente da camada bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf9590c-de07-45e1-a664-bc21bbfe3bbb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770161667795}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "dataframe_bronze = spark.read.format(\"delta\").load(destino_atual + \"/bronze_dim_categoria_produto\")\n",
    "\n",
    "display(dataframe_bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 3 - Processamento para Camada Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
