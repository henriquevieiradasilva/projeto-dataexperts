{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672cd9a8-0a64-4900-9601-f8f68cc25a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 3 - PROCESSAMENTO PARA CAMADA BRONZE**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada bronze, colocando em um formato padrão mas sem mais nenhum refinamento, ainda mantendo os dados brutos.\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5189e80d-cbdc-4c12-b5af-5d412e62fe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    "    BooleanType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b38c156-a91f-4ec5-936b-324b57f4df20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**\n",
    "\n",
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b93818-9cf5-4998-9af0-b1190f26c82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Load Data Bronze\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4ecd4f-680a-4c37-afac-19f694c45665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Definindo Origens, Arquivos e Destinos**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse ` <br>\n",
    "`--> nome_camada_origem --> nome da camada de origem dos dados` <br>\n",
    "`--> nome_volume_origem --> nome do volume de origem dos dados dentro da camada` <br>\n",
    "`--> nome_camada_bronze --> nome da camada de destino dos dados` <br>\n",
    "`--> nome_volume_bronze --> nome do volume de destino dos dados dentro da camada` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28a2622-8e18-4bb2-999c-6e5b18c93766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "nome_camada_origem = \"landing_zone\"\n",
    "nome_volume_origem = \"vendas_atual\"\n",
    "nome_camada_bronze = \"bronze\"\n",
    "nome_volume_bronze = \"vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672b818d-7a41-4de0-b660-4e2b3c31919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6524bea1-6541-4b2a-86f2-fdd233857669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_bronze}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd534185-2e50-4d4e-9765-f9eab33b9742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume de destino caso ele ainda não exista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad938f9-7d50-4430-b4a0-36e419c4161c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_bronze}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f46b91-bb26-4ea6-b6b0-e32703cbdaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira no vetor todos os arquivos que deseja fazer esse processamento para camada bronze:` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d975b5e-91f2-432b-8439-df7ef09481e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7add9c-0e8a-4eaf-bf9f-454df4bd5177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Leitura dos Dados**\n",
    "\n",
    "Para que seja possível manipular os dados, antes precisamos ler todos eles já com uma estrutura pré-definida.\n",
    "\n",
    "`Para cada estrutura, é necessário elaborar manualmente como deve ser interpretada`\n",
    "\n",
    "O código a seguir cria essas estruturas para cada arquivo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76db3fa0-8b17-4855-b1ad-2181380db5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"categoria_produto.csv\": StructType([\n",
    "    StructField(\"categoria_id\", LongType()),\n",
    "    StructField(\"categoria_nome\", StringType())\n",
    "    ]),\n",
    "    \n",
    "    \"cliente.csv\": StructType([\n",
    "    StructField(\"cliente_id\", LongType()),\n",
    "    StructField(\"nome_cliente\", StringType()),\n",
    "    StructField(\"estado\", StringType()),\n",
    "    StructField(\"cidade\", StringType())\n",
    "    ]),\n",
    "\n",
    "    \"data.csv\": StructType([\n",
    "    StructField(\"data_id\", LongType()),\n",
    "    StructField(\"data\", DateType()),\n",
    "    StructField(\"ano\", IntegerType()),\n",
    "    StructField(\"mes\", IntegerType()),\n",
    "    StructField(\"dia\", IntegerType()),\n",
    "    StructField(\"dia_semana\", StringType()),\n",
    "    StructField(\"final_de_semana\", BooleanType())\n",
    "    ]),\n",
    "\n",
    "    \"localidade.csv\": StructType([\n",
    "    StructField(\"localidade_id\", LongType()),\n",
    "    StructField(\"estado\", StringType()),\n",
    "    StructField(\"cidade\", StringType())\n",
    "    ]),\n",
    "\n",
    "    \"produto.csv\": StructType([\n",
    "    StructField(\"produto_id\", LongType()),\n",
    "    StructField(\"preco_lista\", DoubleType()),\n",
    "    StructField(\"categoria_nome\", StringType())\n",
    "    ]),\n",
    "\n",
    "    \"vendas\": StructType([\n",
    "    StructField(\"venda_id\", LongType()),\n",
    "    StructField(\"cliente_id\", LongType()),\n",
    "    StructField(\"produto_id\", LongType()),\n",
    "    StructField(\"data_id\", LongType()),\n",
    "    StructField(\"categoria_id\", LongType()),\n",
    "    StructField(\"localidade_id\", LongType()),\n",
    "    StructField(\"quantidade\", LongType()),\n",
    "    StructField(\"preco_lista\", DoubleType()),\n",
    "    StructField(\"valor_total\", DoubleType())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f00d122-51be-497c-a94f-81b1a955dd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir faz a leitura dos dados em cada estrutura, adicionando a coluna nome do arquivo durante a leitura, e armarzenando o resultado desse processo em um vetor de datas frames para que seja possível manipular esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad0d4aa-c1f4-4caf-ae71-0abb500a9d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    try:\n",
    "        if arquivo.startswith(\"vendas\"):\n",
    "            schema = schemas[\"vendas\"]\n",
    "        else:\n",
    "            schema = schemas[arquivo]\n",
    "\n",
    "        dataframe = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .csv(f\"{origem_dados}/{arquivo}\")\n",
    "        )\n",
    "\n",
    "        dataframes.append(dataframe)\n",
    "        print(f\"Leitura do arquivo {arquivo} realizada com sucesso\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo {arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9de94b-2481-4482-ba2c-18069d1aadde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 5 - **Salvando os Dados no Formato e Local Corretos**\n",
    "\n",
    "Por fim, precisamos criar o parquet com os dados que importamos e com a estrutura que construimos anteriormente e mover para camada bronze.\n",
    "\n",
    "O código a seguir faz essa compressão e envio para o caminho certo (já particionando em ano e mês para ser mais otimizado quando precisarmos utilizar esses dados granularmente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9b5d26-66aa-49e1-953b-d2f13e32597b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dimensoes = [\n",
    "    \"bronze_dim_categoria_produto\",\n",
    "    \"bronze_dim_cliente\",\n",
    "    \"bronze_dim_data\",\n",
    "    \"bronze_dim_localidade\",\n",
    "    \"bronze_dim_produto\"\n",
    "]\n",
    "\n",
    "fato = \"bronze_fato_vendas\"\n",
    "\n",
    "for dataframe, nome in zip(dataframes[:5], dimensoes):\n",
    "    dataframe.write.format(\"delta\").mode(\"overwrite\").save(f\"{destino_dados}/{nome}\")\n",
    "    print(f\"Dimensão {nome} salva com sucesso\")\n",
    "\n",
    "for dataframe in dataframes[5:]:\n",
    "    dataframe.write.format(\"delta\").mode(\"append\").save(f\"{destino_dados}/{fato}\")\n",
    "    print(\"Fato vendas salvo com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36f2d92-973d-42d5-a423-e67ab7d9dac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> O modo de escrita define como os dados serão gravados no destino:\n",
    "- ***overwrite***: remove os dados existentes no caminho/partições e grava tudo novamente\n",
    "- ***append***: adiciona novos dados às partições existentes, sem apagar o que já existe \n",
    "\n",
    "> Em pipelines produtivos, 'append' é o mais comum para cargas incrementais. 'overwrite' costuma ser usado apenas em reprocessamentos completos ou ambientes de teste\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 6 - **Limpeza de Cache e Outros**\n",
    "\n",
    "O código a seguir exclui todos os Data Frames que podem estar em cache no algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c470311-85ab-4434-ae15-5e122826c94e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No Databricks Free Edition (Serverless), o gerenciamento de cache e memória é feito automaticamente pela plataforma. Por esse motivo, alguns comandos tradicionais do Spark não são suportados, como:\n",
    "\n",
    "- dataframe.unpersist()\n",
    "- spark.catalog.clearCache()\n",
    "\n",
    "A alternativa é remover referências aos DataFrames manualmente e forçar a coleta de lixo do Python quando necessário.\n",
    "\"\"\"\n",
    "\n",
    "for dataframe in dataframes:\n",
    "    del dataframe\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799a966e-528a-4009-b5b4-01067af1acfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir monta um novo Data Frame com os dados diretamente da camada bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf9590c-de07-45e1-a664-bc21bbfe3bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "dataframe_bronze = spark.read.format(\"delta\").load(destino_dados + \"/bronze_fato_vendas\")\n",
    "\n",
    "display(dataframe_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683d8bc8-4810-416f-bd6b-dda8eb7691f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "E o código a seguir faz a mesma coisa que o anterior, mas filtrando para mostrar somente mês de janeiro. Mostrando assim a performance mais rápida por ter dividido anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97255de1-a91d-4683-8024-646ac556bdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "\"\"\"\n",
    "dataframe_bronze_janeiro = (\n",
    "    spark.read\n",
    "        .parquet(bronze_destino_dados)\n",
    "        .filter(\n",
    "            (col(\"Ano\") == 2012) &\n",
    "            (col(\"Mes\") == 1)\n",
    "        )\n",
    ")\n",
    "\n",
    "display(dataframe_bronze_janeiro)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 3 - Processamento para Camada Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
