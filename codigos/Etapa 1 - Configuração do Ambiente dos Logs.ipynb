{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fdc965-a718-4165-a398-82017b07824b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 1 - CONFIGURAÇÃO DO AMBIENTE DE LOGS**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora será feito a configuração do ambiente de logs. Logs são registros de tudo que acontece no ambiente, ou seja, são utilizados para manter um histórico de alterações dos ambientes e dos processos.\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240579bc-5f9a-4a3b-8808-010e03d6e3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a3da87-4bb1-4892-906d-467d9395ff78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Criação das Tabelas da Camada de Logs**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse destino` <br>\n",
    "`--> nome_camada --> o nome da camada destinada para logs` <br>\n",
    "`--> nome_tabelas --> todo os nomes das tabelas onde serão armazenados os logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0deb7a-e73f-437d-96fe-ecd6c5933061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "nome_camada = \"audit\"\n",
    "nome_tabelas = [\n",
    "    \"logs\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7336c281-938d-4d4a-b480-1d65965b89ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria as subcamadas se elas ainda não existirem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4053be70-4aa4-4e28-83fd-99a30864a4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for tabela in nome_tabelas:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dataexperts.audit.logs (\n",
    "          log_id STRING,\n",
    "          camada STRING,\n",
    "          job_nome STRING,\n",
    "          acao STRING,\n",
    "          nome_objeto STRING,\n",
    "          fonte STRING,\n",
    "          path_origem STRING,\n",
    "          path_destino STRING,\n",
    "          status STRING,\n",
    "          mensagem_erro STRING,\n",
    "          data_inicio TIMESTAMP,\n",
    "          data_fim TIMESTAMP,\n",
    "          duracao_ms DOUBLE,\n",
    "          registros_lidos LONG,\n",
    "          registros_gravados LONG,\n",
    "          data_execucao DATE\n",
    "      )\n",
    "      USING DELTA\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7987c285-9637-4114-b633-9e593b24580f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir é uma das maneiras simples de se mostrar se deu certo ou não a importação dos arquivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d16c038-980f-433f-9b43-b7c7a6159214",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770093910278}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Resultado Final"
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "display(spark.sql(f\"SHOW TABLES IN {nome_datalakehouse}.{nome_camada}\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 1 - Configuração do Ambiente dos Logs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
