{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0572cd-4c3a-4f47-9c09-aca49018fedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 0 - CRIAÇÃO DO DATA LAKEHOUSE E SUAS CAMADAS**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Após as etapas de _Levantamento de Requisitos_, _Identificação da Base de Dados_, _Geração do Modelo Conceitual_ e _Verificação pós-mesclagem do Modelo Conceitual com a Base de Dados_. Agora começa etapa prática do projeto, iniciando pela criação do ambiente onde ficará organizado os dados no Delta Lakehouse, recomenda-se a estrutura:\n",
    "- **Landing Zone** -> Dados totalmente brutos, em seus formatos originais\n",
    "- **Bronze** -> Dados ainda brutos porém já com formato padrão, geralmente Delta Parquet \n",
    "- **Silver** -> Dados limpos e filtrados\n",
    "- **Gold** -> Dados agrupados, prontos para criação de modelo semânticos\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9897336-abcd-430f-9830-b5c1615d3e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nesse caso não é necessário importar o pyspark, mas em um ambiente python padrão provalmente seria\n",
    "# import pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0888b254-3d64-48a1-b5f0-1cc9a3963335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Criação do Data Lakehouse**\n",
    "\n",
    "`Insira na variável nome_datalakehouse o nome do lakehouse:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5b1a61-7a09-4ef9-a82a-e76241145e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fee8eb-288c-40f1-9855-cbf156f50b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui será criado uma **variável** para ajudar a **validar** se o Data Lakehouse que estamos criando foi realmente criado **nesta execução**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2caaf36f-4dca-45ef-9a35-c53b75b9468e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datalakehouse_foi_criado_agora = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9879dd-ae4c-4ccc-b96b-4d2adc12d405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui é o código para buscar todos os Data Lakehouses existentes e armazenar o resultado dessa consulta em um Data Frame, assim verificando se o nome que estamos tentado criar já existe\n",
    "  - Se já existir, não acontecerá mais no código para não prejudicar outros projetos que possam já existir.\n",
    "  - Se ainda não existir, cria toda a estrutura esperada, tanto o Data Lakehouse quanto as suas respectivas camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b726da7-485e-4e4d-b69f-c56119621673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataslakeshouses_existentes_df = spark.sql(\"SHOW CATALOGS;\")\n",
    "\n",
    "datalakehouse_foi_criado_agora = not (\n",
    "    dataslakeshouses_existentes_df\n",
    "    .filter(dataslakeshouses_existentes_df.catalog == nome_datalakehouse)\n",
    "    .count() > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d80cc3-2819-4071-bbfe-1b3be98c59f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A seguir, o código responsável por **criar** o **catálogo/Data Lakehouse** junto do nome personalizado anteriormente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1743032c-27b3-4196-a472-47633308fc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if datalakehouse_foi_criado_agora:\n",
    "  spark.sql(f\"CREATE CATALOG {nome_datalakehouse};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8993f24e-91ca-4ab4-b2f9-4546381c3b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Usar o Data LakeHouse Criado** \n",
    "\n",
    "O Data Lakehouse já foi criado, mas para que o restante do algoritmo, é necessário **definir explicitamente** qual Data Lakehouse será utilizado.  \n",
    "\n",
    "A seguir, o código que indica o Data Lakehouse que vamos utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cc7590-4a16-4bb2-97f5-9b44a2a43666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if datalakehouse_foi_criado_agora:\n",
    "    spark.sql(f\"USE CATALOG {nome_datalakehouse};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672bd67d-26b8-4499-89b9-26f7d77fdf76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Criação das Partições do Data LakeHouse** \n",
    "\n",
    "`Insira no vetor nomes_das_camadas todos os nomes e quantidade de camadas necessárias para esse caso:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc246306-1026-47b7-b423-3404670c2dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nomes_camadas = [\n",
    "    \"audit\", \n",
    "    \"landing_zone\", \n",
    "    \"bronze\", \n",
    "    \"silver\", \n",
    "    \"gold\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf9440c-69d2-4ca8-9e4c-081590a1c0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A seguir, código para criar as camadas separadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0017a0-702a-4cdb-ba3a-fd84520b5b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if datalakehouse_foi_criado_agora:\n",
    "  for camada in nomes_camadas:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {camada};\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a47814d-cfe0-4445-b467-fbfc73a976a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "Aqui é um log simples para mostrar se deu certo ou não toda a construção da arquitetura planejada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e374b89-5e56-47a0-b555-ed74e6fd8cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "if datalakehouse_foi_criado_agora:\n",
    "    print(f\"[INFO] O Data LakeHouse '{nome_datalakehouse}' foi criado com sucesso!\")\n",
    "else:\n",
    "    print(f\"[INFO] O Data LakeHouse '{nome_datalakehouse}' já existe, escolha outro nome!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 0 - Criação do Data LakeHouse e suas Camadas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
