{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910d20aa-bf59-4e42-accf-d94f259e1381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Demo: Arquitetura Medallion no Databricks\n",
    "Nesta demonstra√ß√£o, vamos simular um pipeline de dados b√°sico utilizando os conceitos de **Camada Bronze** e **Camada Silver**.\n",
    "\n",
    "**O que vamos fazer:**\n",
    "1. **Ingest√£o:** Trazer os dados brutos para o ambiente Spark.\n",
    "2. **Camada Bronze:** Salvar os dados originais com data de recebimento.\n",
    "3. **Camada Silver:** Limpar, padronizar e preparar os dados para an√°lise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27865a34-1c60-4918-8674-c81e424b5c3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Link do Dataset (Cat√°logo de t√≠tulos)\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv\"\n",
    "\n",
    "# 2. Leitura via Pandas (√ötil para coletar dados externos via URL)\n",
    "pdf = pd.read_csv(url)\n",
    "\n",
    "# 3. Convers√£o para Spark DataFrame\n",
    "# Transformamos em Spark para processar grandes volumes de dados de forma distribu√≠da\n",
    "df_raw = spark.createDataFrame(pdf)\n",
    "\n",
    "print(\"‚úÖ Dados carregados com sucesso no Spark!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b88002a-3251-4bb2-92fb-7829ca804ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•â Camada Bronze (Raw)\n",
    "A **Camada Bronze** armazena o dado bruto. \n",
    "- **Regra:** N√£o alteramos o conte√∫do original.\n",
    "- **A√ß√£o:** Apenas adicionamos uma coluna de controle para saber *quando* o dado foi coletado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5baf0c78-b06c-42bc-a43a-4087a4aef73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar o banco de dados se n√£o existir\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# Adicionamos uma coluna de controle (_ingestion_date) para auditoria\n",
    "bronze_df = df_raw.withColumn(\"_ingestion_date\", F.current_timestamp())\n",
    "\n",
    "# Salvando no formato Delta (Padr√£o do Databricks)\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.plotly_raw\")\n",
    "\n",
    "print(\"‚úÖ Tabela BRONZE criada!\")\n",
    "display(spark.table(\"bronze.plotly_raw\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930ab032-30e7-457d-b200-eba4bfda9ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•à Camada Silver (Clean/Refined)\n",
    "Na **Camada Silver**, os dados passam por uma \"filtragem\". O objetivo √© deixar os dados prontos para os analistas de neg√≥cios.\n",
    "\n",
    "**Transforma√ß√µes aplicadas:**\n",
    "1. Padroniza√ß√£o de nomes (snake_case e min√∫sculas).\n",
    "2. Remo√ß√£o de registros completamente vazios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b8102c-7f2a-49d1-9efd-79c8fd9ecd8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepara√ß√£o do ambiente\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "\n",
    "# 2. Leitura da camada anterior (Bronze)\n",
    "bronze_data = spark.table(\"bronze.plotly_raw\")\n",
    "\n",
    "# 3. Limpeza e Padroniza√ß√£o\n",
    "silver_df = bronze_data\n",
    "\n",
    "# Loop para renomear colunas: remove espa√ßos e coloca em min√∫sculas\n",
    "for column in silver_df.columns:\n",
    "    new_name = column.lower().replace(\" \", \"_\")\n",
    "    silver_df = silver_df.withColumnRenamed(column, new_name)\n",
    "\n",
    "# Remove linhas onde todos os valores s√£o nulos\n",
    "silver_df = silver_df.dropna(how=\"all\")\n",
    "\n",
    "# 4. Escrita na Camada Silver\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.plotly_clean\")\n",
    "\n",
    "print(\"‚úÖ Tabela SILVER processada e pronta para uso!\")\n",
    "display(spark.table(\"silver.plotly_clean\").limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "modelo - dataex",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
