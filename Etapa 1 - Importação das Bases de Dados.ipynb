{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d24b12e-3460-481b-b516-df476f060ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 2 - IMPORTAÇÃO DAS BASES DE DADOS**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora será feito a importação dos dados para a camada especificada do Data LakeHouse que já está pronta.\n",
    "\n",
    "*`Esse é um modelo modular, complete as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO**: Esse Notebook foi feito com base na estrutura do Databricks Free Edition, que utiliza catálogos.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bd91bb-80cd-4c6d-afbf-1b92b957b134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7107f0-f8f0-4554-b820-f26b8d9a8fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Importação do Arquivo para o Data Lakehouse**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse destino` <br>\n",
    "`--> nome_camada --> o nome da camada destino dos dados totalmente brutos` <br>\n",
    "`--> nome_volume --> o nome do volume que guardará os arquivos com esses dados`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb457114-ee3f-4b63-b5dc-f654ce7ef169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"dataexperts\"\n",
    "nome_camada = \"landing_zone\"\n",
    "nome_volume = \"vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b1381c-6700-48ec-9e73-6acfb47c7bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume se ele ainda não existir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e5b2b4-de82-4ddf-928f-565292da130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada}.{nome_volume}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef66d7f4-e361-4584-8094-5fa20f716252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`Insira nos vetores todos os nomes dos arquivos que deseja importar e a origem deles, seguindo a mesma ordem:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4464dd7-5d61-4343-bbcc-79ac3e2c43e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "origem_arquivos = [\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "    \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e12fac-e969-441e-ab70-bd32548dce58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir verifica se o arquivo já existe no Data Lakehouse:\n",
    "- Se sim, apenas pula para o próximo download (se ainda exitir arquivos na fila em espera)\n",
    "- Se não, tenta baixar o arquivo da origem.\n",
    "Logo após, caso o download dê certo ou não, passa para a próximo download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd566461-ff7e-446c-8a3f-4d535831c7e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume}/\"\n",
    "\n",
    "for arquivo, origem in zip(arquivos, origem_arquivos):\n",
    "    caminho_destino = os.path.join(destino_dados, arquivo)\n",
    "\n",
    "    if os.path.exists(caminho_destino):\n",
    "        print(f\"[LOG] Arquivo {arquivo} já existe. Pulando download.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(origem + arquivo, caminho_destino)\n",
    "        print(f\"[LOG] Csv {arquivo} baixado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"[LOG] Erro ao baixar o arquivo {arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46b7258-71ad-41ca-91b6-5cd570738067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir é uma das maneiras simples de se mostrar se deu certo ou não a importação dos arquivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4accc836-ecf5-49d4-a4d3-e2615dae5b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "display(dbutils.fs.ls(f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume}/\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1084fdb7-5733-4a33-ab4b-b62fbe9ab502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir mostra um arquivo como exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e112f08-31f2-4287-80ce-d0d2cc3772fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "arquivo_exemplo = f\"/Volumes/{nome_datalakehouse}/{nome_camada}/{nome_volume}/{arquivos[0]}\"\n",
    "\n",
    "dataframe_arquivo_exemplo = spark.read.csv(arquivo_exemplo, header=True)\n",
    "\n",
    "display(dataframe_arquivo_exemplo)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6412521402217558,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 1 - Importação das Bases de Dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
