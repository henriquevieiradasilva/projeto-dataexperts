{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e6b66b5-6346-4a61-a6bb-26e5229b10ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuração final da estrutura de diretórios\n",
    "\n",
    "* data_ex (catalog) \n",
    "    * desafio (schema)\n",
    "        * lhdw (volume)\n",
    "        * bronze (volume)\n",
    "            * bronze_dim_categoria_produto\n",
    "            * bronze_dim_cliente \n",
    "            * bronze_dim_data \n",
    "            * bronze_dim_localidade\n",
    "            * bronze_dim_produto \n",
    "            * bronze_fato_vendas  \n",
    "        * silver (volume)\n",
    "            * silver_dim_categoria_produto\n",
    "            * silver_dim_cliente \n",
    "            * silver_dim_data \n",
    "            * silver_dim_localidade\n",
    "            * silver_dim_produto \n",
    "            * silver_fato_vendas\n",
    "        * gold  (volume)\n",
    "            * dim_categoria_produto\n",
    "            * dim_cliente \n",
    "            * dim_data \n",
    "            * dim_localidade\n",
    "            * dim_produto \n",
    "            * fato_vendas\n",
    "\n",
    "Tornar o escrito a cima em um diagrama para ser um dos anexos ao storytelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645a6909-75a7-4de7-a0b3-b04a7ac507ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"create catalog if not exists data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"create schema if not exists data_ex.desafio\")\n",
    "\n",
    "# Criação dos volumes\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.lhdw\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.bronze\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.silver\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.gold\")\n",
    "\n",
    "# # Criação dos diretórios\n",
    "# paths = [\n",
    "#     \"/Volumes/data_ex/desafio/lhdw/processar\"\n",
    "# ]\n",
    "\n",
    "# for path in paths:\n",
    "#     # Tenta ler os caminhos já existentes\n",
    "#     try:\n",
    "#         dbutils.fs.ls(path)\n",
    "#     # Se o caminho não existir, uma excecao é lancada e um novo caminho é criado\n",
    "#     except:\n",
    "#         dbutils.fs.mkdirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3880c670-75b1-4053-bafe-8ac32b8893dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/Volumes/data_ex/desafio/lhdw/processar\", True)\n",
    "spark.sql(\"drop volume if exists data_ex.desafio.lhdw\")\n",
    "spark.sql(\"drop volume if exists data_ex.desafio.bronze\")\n",
    "spark.sql(\"drop volume if exists data_ex.desafio.silver\")\n",
    "spark.sql(\"drop volume if exists data_ex.desafio.gold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f2127ba-495c-4f5b-a93b-0abb97393980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Importação pelo Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca79ccab-b54b-4fa0-b2ac-74170697df69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a888e6b1-f8c9-4dc4-9ae3-67da3ec6c184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Função para baixar os arquivos\n",
    "def download_dataset(csv_files, url, lhdw_path):\n",
    "    # Tenta realizar o download dos arquivos\n",
    "    try :\n",
    "        for file in csv_files:\n",
    "            # url+file corresponde a url de download\n",
    "            # lhdw_path+file corresponde ao caminho onde o arquivo será salvo(lhdw_path) e, nome o formato do arquivo(file)\n",
    "            urllib.request.urlretrieve(url+file, lhdw_path+file)\n",
    "            print(f\"Csv {file} baixado com sucesso!\")\n",
    "    # Caso não consigua realizar o download, uma exceção é lançada\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {file} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66963c82-1a0a-4036-8873-3d89c3cce682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cec6e163-cc28-4882-9b7d-f11f09304eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilizaçao da função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378baa36-81fe-4034-be13-8b17aa6662b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de .csv\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Reposotório do dataset para o desafio\n",
    "url = \"https://github.com/andrerosa1977/dataexperts2026/blob/main/\" \n",
    "lhdw_path = \"/Volumes/data_ex/desafio/lhdw/\"\n",
    "\n",
    "download_dataset(csv_files, url, lhdw_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97cd979a-2b77-4877-b57e-e2c969659831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/Volumes/data_ex/desafio/lhdw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8cba06a-65ce-4cd7-a220-4d6ecced20fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83da5fa-bf55-4a4f-912c-9511344f507c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do servidor para trabalho no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f314e448-b30c-4b6a-9e40-1bc15c2a95bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f210f36-19a1-4e08-b5f2-95d047b606c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trabalhos no Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e26efd-1131-49e8-b711-e9c0cc46b1e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Definindo o Log de carga\n",
    "* id_carga (string (uuid))\n",
    "* id_job (string (uuid))\n",
    "* nome_arquivo (string)\n",
    "* fonte (string)\n",
    "* camada (string)\n",
    "* path_destino (string)\n",
    "* data_inicio (utc_timestamp)\n",
    "* data_fim (utc_timestamp)\n",
    "* duracao_ms (bigint)\n",
    "* registros_lidos (bigint)\n",
    "* registros_gravados (bigint)\n",
    "* status (string)\n",
    "* mensagem_erro (string)\n",
    "* data_execucao (date(yyyy-MM-dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39fc985-77ff-4719-adb4-8944b0dbe401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas/funções necessárias\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType\n",
    "\n",
    "# Definição dos diretorios de trabalho\n",
    "lhdw_path = \"/Volumes/data_ex/desafio/lhdw\"\n",
    "bronze_path = \"/Volumes/data_ex/desafio/bronze\"\n",
    "\n",
    "# Arquivos csv a serem trabalhados\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Lista dos nomes dos schemas em ordem de execução\n",
    "schemas = [\n",
    "    \"schema_categoria\",\n",
    "    \"schema_cliente\",\n",
    "    \"schema_data\",\n",
    "    \"schema_localidade\",\n",
    "    \"schema_produto\",\n",
    "    \"schema_vendas\"\n",
    "]\n",
    "\n",
    "# Definição manual do schemas a serem aplicados nos arquivos csv (futuros parquets)\n",
    "schema_categoria = StructType([\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True),\n",
    "])\n",
    "\n",
    "schema_cliente = StructType([\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"nome_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_data = StructType([\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"data\", DateType(), True),\n",
    "    StructField(\"ano\", IntegerType(), True),\n",
    "    StructField(\"mes\", IntegerType(), True),\n",
    "    StructField(\"dia\", IntegerType(), True),\n",
    "    StructField(\"dia_semana\", StringType(), True), \n",
    "    StructField(\"final_de_semana\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "schema_localidade = StructType([\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    ])  \n",
    "\n",
    "schema_produto = StructType([\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"venda_id\", LongType(), True),\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"quantidade\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2150e576-d26b-4429-a8da-0ab472fe8090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Criando uma lista para armazenar os Sparks DataFrames\n",
    "spark_df = []\n",
    "\n",
    "# Criação do Spark DataFrame, leitura dos csv, aplicação dos schemas e escrita em parquet \n",
    "def bronze_work(csv_file, schemas):\n",
    "    for file, schema in zip(csv_files, schemas):\n",
    "        # Tenta ler os arquivos csv em lhdw_path\n",
    "        try:\n",
    "            # Se schema não for vendas, aplica o schema correspondente a file\n",
    "            if schema is not \"schema_vendas\":\n",
    "                df = spark.read.csv(f\"{lhdw_path}/{file}\", header=True, schema=schema)\n",
    "                print(f\"Leitura do arquivo {file} bem suceddida.\")\n",
    "            \n",
    "            # Se for vendas, deve repetir a leirura do arquivo n vezes\n",
    "            else:\n",
    "                # Tenta ler o caminho no qual vendas esta\n",
    "                try:\n",
    "                    paths = dbutils.fs.ls(lhdw_path)\n",
    "                    # Conta o numero csv vendas existentes\n",
    "                    n = len([n for n in paths if n.name.startswith(\"vendas\")])\n",
    "                    # Aplica um loop para ler todos os arquivos vendas\n",
    "                    for i in range(1, n+1):\n",
    "                        try:\n",
    "                            df = spark.read.csv(f\"{lhdw_path}/vendas_part{i}.csv\", header=True, schema=schema)\n",
    "                            print(f\"Leitura do arquivo vendas_part{i}.csv bem suceddida.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erro ao ler o arquivo {lhdw_path}/vendas_part{i}.csv: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(\"ERRO\")\n",
    "                    print(f\"Erro ao ler o caminho no qual no qual vendas esta {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "bronze_work(csv_files, schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff77e76-9a2c-4406-bc54-cd4fcd7cfc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvar no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9f228c-4a0e-41b9-abe4-3002f44e886c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de682b3-9be7-4aa9-876e-7e920134be33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limpar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "749206e6-356b-4ec1-b63c-f05ba0a5f7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
