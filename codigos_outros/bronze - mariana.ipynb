{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6b16d4-4ea7-4fd4-a49e-b4f9eb028503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer: Configurações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35a1170-9fe1-4905-8682-b135f3a33635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4501328-f213-4ab0-9062-be486f6962a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import gc\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType, ByteType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217555cf-c1bb-4f54-b973-8f07411ffd1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração da estrutura do Unity catalog\n",
    "\n",
    "* data_ex (catalog) \n",
    "    * metadados (schema)\n",
    "        * meta_bronze (table)\n",
    "        * meta_silver (table)\n",
    "        * meta_gold (table)\n",
    "    * lhdw (schema) \n",
    "        * versao001 (volumes)\n",
    "        * versao002 (volumes)\n",
    "        * versao003 (volumes)       \n",
    "    * bronze (schema)\n",
    "        * bronze_log_carga (table)\n",
    "        * versao003 (volume)\n",
    "            * bronze_dim_categoria_produto\n",
    "            * bronze_dim_cliente \n",
    "            * bronze_dim_data \n",
    "            * bronze_dim_localidade\n",
    "            * bronze_dim_produto \n",
    "            * bronze_fato_vendas  \n",
    "    * silver (schema)\n",
    "        * versao* (volume)\n",
    "            * silver_dim_categoria_produto\n",
    "            * silver_dim_cliente \n",
    "            * silver_dim_data \n",
    "            * silver_dim_localidade\n",
    "            * silver_dim_produto \n",
    "            * silver_fato_vendas\n",
    "    * gold  (schema)\n",
    "        * versao* (volume)\n",
    "            * dim_categoria_produto\n",
    "            * dim_cliente \n",
    "            * dim_data \n",
    "            * dim_localidade\n",
    "            * dim_produto \n",
    "            * fato_vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5564525-7478-4127-b1d5-521e77d90c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"create catalog if not exists data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"create schema if not exists data_ex.metadados\")\n",
    "spark.sql(\"create schema if not exists data_ex.lhdw\")\n",
    "spark.sql(\"create schema if not exists data_ex.bronze\")\n",
    "spark.sql(\"create schema if not exists data_ex.silver\")\n",
    "spark.sql(\"create schema if not exists data_ex.gold\")\n",
    "\n",
    "# Criação das tabelas de metadados fixos, para controle de job \n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_bronze (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_silver (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_gold  (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "# Abaixo são geradas uuids do tipo 4, a qual não requer argumentos e é gerada aleatoriamente\n",
    "id_job_bronze = str(uuid.uuid4())\n",
    "id_job_silver = str(uuid.uuid4())\n",
    "id_job_gold = str(uuid.uuid4())\n",
    "\n",
    "# Armazena a uuid gerada na respectiva camada\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_bronze values ('{id_job_bronze}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_silver values ('{id_job_silver}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_gold values ('{id_job_gold}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29a666b3-66bf-4937-9a81-21e3e34a5fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do servidor para trabalho no Bronze Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf2641d-8ea8-46e1-ae89-77b6207f9eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Otimização retirada do material disponibilizado do curso de Databricks\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce401f99-9f2f-4755-96fb-9ec462835a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções do Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c676bf2-bef3-4958-843b-9536bdce66f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### download()\n",
    "Retorna o nome do volum em que o download foi feito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cfc712-ae46-4cff-af0b-12f1ca242733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para baixar os arquivos\n",
    "def download_dataset(csv_files, url, path):\n",
    "    # Tenta realizar o download dos arquivos\n",
    "    try :\n",
    "        # Variável para armazenar o nome do novo volume\n",
    "        volume = \"\"\n",
    "        try:\n",
    "            # Conta quantos volumes já existem dentro desse do schema\n",
    "            numero = spark.sql(\"show volumes in data_ex.lhdw\").count()\n",
    "            # Cria o comando sql para criar um novo volume para a nova versão do download\n",
    "            volume = f\"download{numero+1:03d}\"\n",
    "            sql_command = f\"create volume if not exists data_ex.lhdw.{volume}\"\n",
    "            # Executa o comando sql acima\n",
    "            spark.sql(sql_command)\n",
    "        # Lança um exceção caso houver um erro\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar o volume: {e}\")\n",
    "        for file in csv_files:\n",
    "            down_path = f\"{url+file}\"\n",
    "            up_path = f\"{path+volume}/{file}\"\n",
    "            urllib.request.urlretrieve(down_path, up_path)\n",
    "            print(f\"Csv {file} baixado com sucesso!\")\n",
    "    # Caso não consiga realizar o download, uma exceção é lançada\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {file} - {e}\")\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d3a1425-bb88-4cf8-a14d-5687f425ee1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### read_dim()\n",
    "Função para ler as dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8a7c47-6c0a-4a55-83b3-eadef874c1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_dim(path, file, schema):\n",
    "    try:\n",
    "        df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .schema(schema)\\\n",
    "            .csv(f\"{path}/{file}\")\n",
    "        return df\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87fdd3d-fb6c-4e05-b593-9b7ebc823509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### save_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1427e674-0407-491b-8b14-8c8f3d0efae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_dim(df_read, dime, camada):\n",
    "    try:\n",
    "        df_fato = df_read.write.format(\"delta\").mode(\"overwrite\").save(f\"{camada}/{dime}\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ec35d5a-c362-497f-b1c8-cecf29acc2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### read_fato()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa0b44e-4fe4-437e-ab9a-5083d8afae54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_fato(path, files, schema, n):\n",
    "    print(\"entrou na read_fato()\")\n",
    "    try:\n",
    "        vendas_df = [0]\n",
    "        # Aplica um loop para ler todos os arquivos vendas\n",
    "        for i in range(0, n):\n",
    "            try:\n",
    "                df = spark.read.option(\"header\", \"true\")\\\n",
    "                    .schema(schema_vendas)\\\n",
    "                    .csv(f\"{path}/{files[i]}\")\n",
    "                vendas_df[0] = vendas_df[0] + df.count()\n",
    "                vendas_df.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler o arquivo {path}/{files[i]}: {e}\")\n",
    "        return vendas_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo {files[i]}: {e}\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ef3727-c114-4016-8cc6-fbd5a1d0948c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### save_fato()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cf31a0-ba2c-48bf-8e04-c84d03135d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_fato(df, fato, n, camada):\n",
    "    #Loop para criar a tabela de fato vendas\n",
    "    for i in range(0, n):\n",
    "        try:\n",
    "            df[i].write.format(\"delta\").mode(\"append\").save(f\"{camada}/{fato}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe vendas: {e}\")\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f931839f-2c26-444d-bc93-6a1dd6be9d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### makeLogTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905bf037-7d95-46a8-8c07-cb85e9a54904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def makeLogTable():\n",
    "    n = 0\n",
    "    try:\n",
    "        n = spark.sql(\"show tables in data_ex.bronze\").count()\n",
    "    except:\n",
    "        print(\"ERRO ao ler a tabela de logs\")\n",
    "    if n == 0:\n",
    "        try :\n",
    "            spark.sql(\"\"\"create table if not exists data_ex.bronze.bronze_log_carga (\n",
    "                id_carga string,\n",
    "                id_job string,\n",
    "                nome_arquivo string,\n",
    "                fonte string,\n",
    "                camada string,\n",
    "                path_origem string,\n",
    "                path_destino string,\n",
    "                data_inicio timestamp,\n",
    "                data_fim timestamp,\n",
    "                duracao_ms string,\n",
    "                registros_lidos integer,\n",
    "                registros_gravados integer,\n",
    "                status string,\n",
    "                mensagem_erro string,\n",
    "                data_execusao date\n",
    "                ) using delta \"\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar a tabela de logs: {e}\")\n",
    "    elif n > 1:\n",
    "        print(f\"Erro na árvore de diretórios, exite mais de {n} tabelas de logs na camada bronze\")\n",
    "    else:\n",
    "        print(\"Tabela de logs já existe na camada bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450a4bd0-2733-4375-9da4-0adafaa3c5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### generate_pre_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7f3e5f-b2f1-4f60-b402-63ce049d64b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_pre_log(file, dime, volume_salvo):\n",
    "  try:\n",
    "    id_carga = str(uuid.uuid4())\n",
    "    id_job   = spark.sql(\"select * from data_ex.metadados.meta_bronze\").collect()[0][0]\n",
    "    nome_arquivo = dime\n",
    "    fonte = \"filesystem_local\"\n",
    "    camada = \"bronze\"\n",
    "    path_origem = f\"/Volumes/data_ex/lhdw/{volume_salvo}/{file}\"\n",
    "    path_destino = f\"/Volumes/data_ex/bronze/{volume_salvo}/{dime}\"\n",
    "    data_inicio = spark.range(1).select(current_timestamp()).collect()[0][0]\n",
    "    data_fim = \"\"\n",
    "    dureação_ms = \"\"\n",
    "    registros_lidos = \"\"\n",
    "    registros_gravados = \"\"\n",
    "    status = \"Running\"\n",
    "    mensagem_erro = \"null\"\n",
    "    data_execusao = spark.range(1).select(current_date()).collect()[0][0]\n",
    "    return [id_carga, id_job, nome_arquivo, fonte, camada, path_origem, path_destino, data_inicio, data_fim, dureação_ms, registros_lidos, registros_gravados, status, mensagem_erro, data_execusao]\n",
    "  except Exception as e:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f09d6b-e2f6-454a-89ff-e01c055cffc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### gerarLog(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa94a4a-3c78-4915-914b-1d08bfed619a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def gerarLog(log_data):\n",
    "    try:\n",
    "        log_data[12] = \"True\"   # Atualiza o status do log\n",
    "        data_fim = spark.range(1).select(current_timestamp()).collect()[0][0]\n",
    "        log_data[8] = data_fim  \n",
    "        duracao_ms = (data_fim - log_data[7]).total_seconds() * 1000\n",
    "        log_data[9] = duracao_ms\n",
    "        acao = storeLog(log_data)\n",
    "        if acao == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na função gerarLog(): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e25a16-a4d2-4015-a07e-c31a82100aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### storeLog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de51de92-5fc7-453b-b159-4d8dde0bf5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def storeLog(log_data):\n",
    "    try:\n",
    "        row = Row(\n",
    "            id_carga = log_data[0],\n",
    "            id_job = log_data[1],\n",
    "            nome_arquivo = log_data[2],\n",
    "            fonte = log_data[3],\n",
    "            camada = log_data[4],\n",
    "            path_origem = log_data[5],\n",
    "            path_destino = log_data[6],\n",
    "            data_inicio = log_data[7],\n",
    "            data_fim = log_data[8],\n",
    "            duracao_ms = log_data[9],\n",
    "            registros_lidos = log_data[10],\n",
    "            registros_gravados = log_data[11],\n",
    "            status = log_data[12],\n",
    "            mensagem_erro = log_data[13],\n",
    "            data_carga = log_data[14]\n",
    "        )\n",
    "\n",
    "        df_log = spark.createDataFrame([row])\n",
    "        df_log.write.mode(\"append\").insertInto(\"data_ex.bronze.bronze_log_carga\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gravar o log: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b61aaed-382b-4bbf-84cd-41d6e328ce97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções de leitura e escrita "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656bdcd9-543b-43e8-b98e-084a99e3e088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do Spark DataFrame, leitura dos csv, aplicação dos schemas e escrita em parquet \n",
    "def bronze_work_dim(csv_files, schemas, dime, camada, n):\n",
    "  files = csv_files[:-4]\n",
    "  # Índice para registrar o nome da dimensão a ser salvada\n",
    "  for i, (file, schema) in enumerate(zip(files, schemas)):\n",
    "    # Tenta ler os arquivos csv em lhdw_path\n",
    "    try:\n",
    "      # Inicia o registo de logs\n",
    "      try: \n",
    "        log_data = generate_pre_log(file, dime[i], volume_salvo)\n",
    "        if log_data != 0: \n",
    "          # Se schema não for vendas, aplica o schema correspondente a file\n",
    "          if schema != schema_vendas:\n",
    "            # Tenta chamar a função para ler um arquivo de não vendas\n",
    "            df_read = read_dim(lhdw_path, file, schema)\n",
    "            if df_read == 0:\n",
    "              printf(f\"Erro ao ler o arquivo {file}\")\n",
    "            else:\n",
    "              # Lê o arquivo csv\n",
    "              registros_lidos = df_read.count()\n",
    "              log_data[10] = registros_lidos\n",
    "              # Silver/Gold process\n",
    "\n",
    "              # Salva\n",
    "              save = save_dim(df_read, dime[i], camada)\n",
    "              if save == 0:\n",
    "                printf(f\"Erro ao salvar o dataframe \")\n",
    "              else:\n",
    "                # Salvamento concluído, armazenando o log respectivo\n",
    "                registros_gravados = spark.read.format(\"delta\").load(f\"{camada}/{dime[i]}\").count()\n",
    "                log_data[11] = registros_gravados\n",
    "                try:\n",
    "                  acao = gerarLog(log_data)\n",
    "                  if acao == 1:\n",
    "                    print(f\"DataFrame salvo com sucesso em {camada}/{dime[i]}\")\n",
    "                  else:\n",
    "                    printf(\"ERRO ao gravar o log\")\n",
    "                except Exception as e:\n",
    "                  print(f\"Erro ao gerar o log e armazená-lo, função gerarLog(): {e}\")\n",
    "          else:\n",
    "            print(\"Não foi possível gerar o pre_log\")\n",
    "        else:\n",
    "          printf(\"ERRO ao gerar log\")\n",
    "      except Exception as e:\n",
    "        print(f\"Erro ao chamar o log: {e}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Erro ao iniciar a função de leitura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92559ccf-a951-4cf3-8100-3a92655513ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bronze_work_fato(csv_files, schemas, camada, fato_name, volume_salvo, n):\n",
    "    try:\n",
    "        log_data = generate_pre_log(\"vendas\", fato_name, volume_salvo)\n",
    "        # Chama a função de leitura\n",
    "        df_read = read_fato(lhdw_path, csv_files[-4:] ,schema_vendas, n)\n",
    "\n",
    "        if df_read == 0:\n",
    "            print(f\"Erro ao ler os arquivos de vendas\")\n",
    "        else:\n",
    "            registros_lidos = df_read[0]\n",
    "            log_data[10] = registros_lidos\n",
    "            df_read.pop(0)\n",
    "            try:\n",
    "                # Salvar\n",
    "                df_fato = save_fato(df_read, fato_name, n, camada)\n",
    "            except Exception as e:\n",
    "                printf(f\"Erro ao chamar a função save_fato() : {e}\")\n",
    "            if df_fato == 0:\n",
    "                printf(f\"Erro ao salvar o dataframe \")\n",
    "            else:\n",
    "                # Salvamento concluído, armazenando o log respectivo\n",
    "                registros_gravados = spark.read.format(\"delta\").load(f\"{camada}/{fato_name}\").count()\n",
    "                log_data[11] = registros_gravados\n",
    "                acao = gerarLog(log_data)\n",
    "    except Exception as e:\n",
    "                print(f\"Erro ao ler o arquivo!!: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f744f64a-ec87-4eea-b3cb-b928f994ef80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chamada das funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d501a5ac-37e1-4e7a-b56a-82717a346ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importação e definição de variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e67561d-dd41-4818-8f1d-251484960dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Importação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bd2ae3-37c7-4e51-853f-c7cf2fb7aacc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de .csv a serem lidos\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Reposotório do dataset para o desafio\n",
    "url = \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\"\n",
    "\n",
    "# Path para o schema de armazenamento, sem o volume\n",
    "lhdw_path = \"/Volumes/data_ex/lhdw/\"\n",
    "\n",
    "# Realiza o download dos arquivos e recebe o nome do volume salvo\n",
    "volume_salvo = download_dataset(csv_files, url, lhdw_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "959f9712-0728-44d1-9c44-d01bcb1ada3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(volume_salvo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbc1e0b-1045-4846-a0c4-1b5ee948a93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Atualizando os caminhos com o volume obtido e definido os nomes das tabelas que serão salvas no Bronze Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45e7a2b-2eb9-4f38-a4db-5aa8d32a2ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição dos diretórios de leitura\n",
    "lhdw_path = f\"{lhdw_path}/{volume_salvo}\"\n",
    "\n",
    "sql_command = f\"create volume if not exists data_ex.bronze.{volume_salvo}\"\n",
    "# Criação do volume no schema bronze\n",
    "spark.sql(sql_command)\n",
    "\n",
    "# Definição do path bronze\n",
    "bronze_path = f\"/Volumes/data_ex/bronze/{volume_salvo}\"\n",
    "\n",
    "# Lista de nomes das dimensões bronze\n",
    "dim_name = [\n",
    "  \"bronze_dim_categoria_produto\",\n",
    "  \"bronze_dim_cliente\",\n",
    "  \"bronze_dim_data\", \n",
    "  \"bronze_dim_localidade\",\n",
    "  \"bronze_dim_produto\", \n",
    "]\n",
    "\n",
    "# Nome da tabela fato\n",
    "fato_name = \"bronze_fato_vendas\"\n",
    "\n",
    "# Numero de csv que fazem parte da bronze fato\n",
    "n_fatos = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9035d35-e64e-4370-8354-d158a6066b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Definição manual dos schemas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f76ecf-6ddf-48b5-967f-142404a631f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista dos nomes dos schemas em ordem de execução\n",
    "schemas = []\n",
    "\n",
    "# Definição manual do schemas a serem aplicados nos arquivos csv (futuros parquets)\n",
    "schema_categoria = StructType([\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True),\n",
    "])\n",
    "schemas.append(schema_categoria)\n",
    "\n",
    "schema_cliente = StructType([\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"nome_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_cliente)\n",
    "\n",
    "schema_data = StructType([\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"data\", DateType(), True),\n",
    "    StructField(\"ano\", IntegerType(), True),\n",
    "    StructField(\"mes\", IntegerType(), True),\n",
    "    StructField(\"dia\", IntegerType(), True),\n",
    "    StructField(\"dia_semana\", StringType(), True), \n",
    "    StructField(\"final_de_semana\", ByteType(), True)\n",
    "])\n",
    "schemas.append(schema_data)\n",
    "\n",
    "schema_localidade = StructType([\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    ])  \n",
    "schemas.append(schema_localidade)\n",
    "\n",
    "schema_produto = StructType([\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_produto)\n",
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"venda_id\", LongType(), True),\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"quantidade\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True)\n",
    "])\n",
    "schemas.append(schema_vendas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "591fa819-cfc9-4dea-8a1c-3957ce6b0a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operações coma as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e348d6b5-2ce3-4658-bdc1-0046a0f27d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Garantindo a existencia da Bronze log table\n",
    "makeLogTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef51574-e6e1-4241-9fa9-d9819c88aa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Executando a função principal da dim\n",
    "bronze_work_dim(csv_files, schemas, dim_name, bronze_path, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20916332-dde0-4942-8768-6f83929e7e83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Executando os fatos\n",
    "bronze_work_fato(csv_files, schemas, bronze_path, fato_name, volume_salvo, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b83e6e9-ef80-4d52-a067-a34bb3b383f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from data_ex.bronze.bronze_log_carga\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bdf066e-8013-4483-a86c-a2921279bfd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limpar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63418df8-857f-40b7-b130-36aa5bccdc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ce4c7f-b62f-4ea7-9861-5c332e2bb7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02cbcff-07eb-44b9-8b52-7f96c2e414f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "spark.sql(\"drop schema if  exists data_ex.lhdw cascade\")\n",
    "spark.sql(\"drop schema if  exists data_ex.bronze cascade\")\n",
    "spark.sql(\"drop schema if  exists data_ex.metadados cascade\")\n",
    "spark.sql(\"drop schema if  exists data_ex.silver cascade\")\n",
    "spark.sql(\"drop schema if  exists data_ex.gold cascade\")\n",
    "spark.sql(\"drop table if exists data_ex.metadados.meta_bronze\")\n",
    "spark.sql(\"drop table if exists data_ex.metadados.meta_silver\")\n",
    "spark.sql(\"drop table if exists data_ex.metadados.meta_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d1b09c3-3a80-4819-85b4-b08417591070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extra: Printar tabelas bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2e5119-bc3f-416c-9e6b-ce557387eebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bronze_dim_categoria_produto = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_dim_categoria_produto\")\n",
    "display(bronze_dim_categoria_produto)\n",
    "\n",
    "bronze_dim_cliente = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_dim_cliente\")\n",
    "display(bronze_dim_cliente)\n",
    "\n",
    "bronze_dim_data = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_dim_data\")\n",
    "display(bronze_dim_data)\n",
    "\n",
    "bronze_dim_localidade = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_dim_localidade\")\n",
    "display(bronze_dim_localidade)\n",
    "\n",
    "bronze_dim_produto = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_dim_produto\")\n",
    "display(bronze_dim_produto)\n",
    "\n",
    "bronze_fato_vendas = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/bronze/{volume_salvo}/bronze_fato_vendas\")\n",
    "display(bronze_fato_vendas)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze - mariana",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
