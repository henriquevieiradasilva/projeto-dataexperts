{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1722eb-779c-4301-a644-f2259f1e2c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db5b96b-a7d6-459b-beac-6e4651e1ee6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do servidor para trabalho no Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b35a6ea-d951-4709-9309-d0becf9cfa9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import uuid\n",
    "import gc\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType \n",
    "\n",
    "# Create a SparkSession with the required configurations for Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Carga Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f952ddf7-d981-40ef-9237-bbeee9838313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791318d6-3f4c-46ef-8cf4-7947fdc2003b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções para verificar o id_job e caminhos.\n",
    "Com execeção dos paths dentro do schema gold, qualquer erro levantado nas funções abaixo é com relação a processos de outras camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5901b83b-b311-4cfc-8ee3-771bae67b761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verifica se os caminhos para a camada gold existem\n",
    "def gold_volume():\n",
    "    try:\n",
    "        spark.sql(\"create schema if not exists data_ex.gold\")\n",
    "        if not spark.sql(\"show volumes in data_ex.gold\").collect():\n",
    "            spark.sql(f\"create volume if not exists data_ex.gold.main\")\n",
    "            print(\"Volume criado com sucesso.\")\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"Volume já existe.\")\n",
    "            return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na função gol_directories(): {e}\") \n",
    "        return 0\n",
    "\n",
    "# verifica se os caminhos para a camada silver existem\n",
    "def exist_silver_path(volume):\n",
    "    volume = volume.lower()\n",
    "    try:\n",
    "        command = spark.sql(f\"show volumes in data_ex.silver\").collect() \n",
    "        if not command:  \n",
    "            print(f\"Não existem volumes na camada silver\")\n",
    "            return 0\n",
    "        else:\n",
    "            if command[0][1] == volume:\n",
    "                print(f\"O volume {volume} existe na camada silver\")\n",
    "                return 1\n",
    "            print(f\"O volume {volume} não existe na camada silver data_ex.silver\")\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na função exist_silver_path(): {e})\")\n",
    "        return 0\n",
    "\n",
    "# Verifica de o id_job existe no metadados\n",
    "def check_jo_id(camada):\n",
    "    relacoes = {\n",
    "        \"bronze\" : \"meta_bronze\",\n",
    "        \"silver\" : \"meta_silver\",\n",
    "        \"gold\" : \"meta_gold\"\n",
    "    }\n",
    "    try:\n",
    "        if not spark.sql(f\"select * from data_ex.metadados.{relacoes[camada]}\"):\n",
    "            print(f\"id_job para a camada {camada} inesiste, gere um novo id_job para a camada {camada}\")\n",
    "            return 0\n",
    "        else:\n",
    "            print(f\"id_job para a camada {camada} existe\")\n",
    "            return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na função check_jo_id(): {e})\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "556e4167-811a-42d4-a548-6cdaa9713bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (df || 0) read(path, volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f172a916-fcc1-41a1-b90e-92a9385d365d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read(volume, file_name):\n",
    "    try:\n",
    "        df = spark.read\\\n",
    "            .format(\"delta\")\\\n",
    "            .load(f\"/Volumes/data_ex/silver/{volume}/{file_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao tentar ler {file_name}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a01c775a-a844-4ebe-b6ba-af8a8247a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções de log de carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b296dcfb-7512-4304-b845-22c733a0df53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_log_tabel(camada):\n",
    "    relacoes = {\n",
    "            \"bronze\" : \"bronze_log_carga\",\n",
    "            \"silver\" : \"silver_log_carga\",\n",
    "            \"gold\"   : \"gold_log_carga\"\n",
    "        }\n",
    "    n = 0\n",
    "    # Conta quantas tabelas existem\n",
    "    try:\n",
    "        n = spark.sql(\"show tables in data_ex.bronze\").count()\n",
    "    except:\n",
    "        print(\"ERRO ao ler a tabela de logs\")\n",
    "    # Cria a tabela se não existir \n",
    "    if n == 0:\n",
    "        try :\n",
    "            spark.sql(f\"\"\"create table if not exists data_ex.bronze.{relacoes[camada]} (\n",
    "                id_carga string,\n",
    "                id_job string,\n",
    "                nome_arquivo string,\n",
    "                fonte string,\n",
    "                camada string,\n",
    "                path_origem string,\n",
    "                path_destino string,\n",
    "                data_inicio timestamp,\n",
    "                data_fim timestamp,\n",
    "                duracao_ms string,\n",
    "                registros_lidos integer,\n",
    "                registros_gravados integer,\n",
    "                status string,\n",
    "                mensagem_erro string,\n",
    "                data_execusao date\n",
    "                ) using delta \"\"\")\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar a tabela de logs: {e}\")\n",
    "            return 0\n",
    "    # Erro, existe mais de uma tabela\n",
    "    elif n > 1:\n",
    "        print(f\"Erro na árvore de diretórios, exite mais de {n} tabelas de losgs na camada bronze\")\n",
    "        return 0\n",
    "    # A tabela ja existe\n",
    "    else:\n",
    "        print(\"Tabela de logs ja existes na camada bronze\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ab76dd-630b-4273-839b-241389c6ce56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_log(volume, camada_destino, file, new_name):\n",
    "  camada_origem = \"silver\"\n",
    "  try:\n",
    "    global log_data\n",
    "    log_data.update({\n",
    "      \"id_carga\" : str(uuid.uuid4()),\n",
    "      \"id_job\"   : spark.sql(\"select * from data_ex.metadados.meta_bronze\").collect()[0][0],\n",
    "      \"nome_arquivo\" : new_name,\n",
    "      \"fonte\" : \"filesystem_local\",\n",
    "      \"camada\" : \"bronze\",\n",
    "      \"path_origem\" : f\"/Volumes/data_ex/{camada_origem}/{volume}/{file}\",\n",
    "      \"path_destino\" : f\"/Volumes/data_ex/{camada_destino}/{volume}/{new_name}\",\n",
    "      \"data_inicio\" : spark.range(1).select(current_timestamp()).collect()[0][0],\n",
    "      \"status\" : \"Running\",\n",
    "      \"data_execusao\" : spark.range(1).select(current_date()).collect()[0][0]\n",
    "    })\n",
    "    return 1\n",
    "  except Exception as e:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58d672c-3f5b-427e-9c5b-d7902a649958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_log():\n",
    "    try:\n",
    "        global log_data\n",
    "        log_data[\"status\"] = \"True\"   # Atualiza o status do log\n",
    "        data_fim = spark.range(1).select(current_timestamp()).collect()[0][0]\n",
    "        log_data[\"data_fim\"] = data_fim  \n",
    "        duracao_ms = (data_fim - log_data[7]).total_seconds() * 1000\n",
    "        log_data[\"duracao_ms\"] = duracao_ms\n",
    "        acao = storeLog(log_data)\n",
    "        if acao == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na funcao gerarLog(): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b59d4eb-ebca-4029-86c0-71046ec4f577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def storeLog():\n",
    "    try:\n",
    "        global log_data\n",
    "        row = Row(\n",
    "            id_carga = log_data[\"id_carga\"],\n",
    "            id_job = log_data[\"id_job\"],\n",
    "            nome_arquivo = log_data[\"nome_arquivo\"],\n",
    "            fonte = log_data[\"fonte\"],\n",
    "            camada = log_data[\"camada\"],\n",
    "            path_origem = log_data[\"path_origem\"],\n",
    "            path_destino = log_data[\"path_destino\"],\n",
    "            data_inicio = log_data[\"data_inicio\"],\n",
    "            data_fim = log_data[\"data_fim\"],\n",
    "            duracao_ms = log_data[\"duracao_ms\"],\n",
    "            registros_lidos = log_data[\"registros_lidos\"],\n",
    "            registros_gravados = log_data[\"registros_gravados\"],\n",
    "            status = log_data[\"status\"],\n",
    "            mensagem_erro = log_data[\"mensagem_erro\"],\n",
    "            data_carga = log_data[\"data_carga\"]\n",
    "        )\n",
    "\n",
    "        df_log = spark.createDataFrame([row])\n",
    "        df_log.write.mode(\"append\").insertInto(\"data_ex.bronze.bronze_log_carga\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gravar o log: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef114f6-6f9c-49ad-a7cc-44a4a37197c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (df || 0) save(df, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040dd48c-7f52-4e1f-8c93-90bc2a1b7446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_table(df, new_file_name):\n",
    "    try:\n",
    "        if \"fato\" in new_file_name:\n",
    "            df_data = spark.read.format(\"delta\").load(\"/Volumes/data_ex/gold/main/dim_data\")\n",
    "            \n",
    "            df_fato = df\\\n",
    "                .join(broadcast(\n",
    "                    df_data.select(\"sk_data\", \"ano\", \"mes\")),\n",
    "                      \"sk_data\")\\\n",
    "            \n",
    "            df_fato\\\n",
    "                .write\\\n",
    "                .format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .partitionBy(\"ano\",\"mes\")\\\n",
    "                .saveAsTable(f\"data_ex.gold.{new_file_name}\")\n",
    "            \n",
    "            df_fato\\\n",
    "                .write\\\n",
    "                .format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .save(f\"/Volumes/data_ex/gold/main/{new_file_name}\")\n",
    "            \n",
    "            return df_fato\n",
    "        else:\n",
    "            df.write\\\n",
    "                .format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .saveAsTable(f\"data_ex.gold.{new_file_name}\")\n",
    "            df.write\\\n",
    "                .format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .save(f\"/Volumes/data_ex/gold/main/{new_file_name}\")\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na função save(): {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cefb4af-3531-4506-9103-2f349b3d5ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### execute_gold_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e1cf9b-7d08-4187-b6e1-545f3a4dca1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de logs\n",
    "log_data = {}\n",
    "\n",
    "def execute_gold_layer(volume, camada, new_files_names):\n",
    "\n",
    "    # Garante que as pastas no unity catalog gold existam\n",
    "    if not gold_volume():\n",
    "        return\n",
    "\n",
    "    # Garante a existencia da tabelas de logs de acesso da camada gold\n",
    "    if not make_log_tabel(camada):\n",
    "        return\n",
    "\n",
    "    # Verifica se existe o caminho até a camada silver\n",
    "    if not exist_silver_path(volume):\n",
    "        return\n",
    "    \n",
    "    # Verifica se existe o id_job no metadados\n",
    "    if not check_jo_id(camada):\n",
    "        return\n",
    "    \n",
    "    files = listdir(\"/Volumes/data_ex/silver/download001\")\n",
    "    \n",
    "    for i, (file, new_file_name) in enumerate(zip(files,new_files_names)): \n",
    "\n",
    "        # Gerar o log\n",
    "        generate_log(volume, camada, file, new_file_name)\n",
    "\n",
    "        # Ler o arquivo\n",
    "        df_read = read(volume, file)\n",
    "        \n",
    "        # df_read e nulo ou 0\n",
    "        if not df_read:\n",
    "            return\n",
    "        \n",
    "        # df_read é um dataframe nao nulo, assim, podemos contar os registros lidos\n",
    "        global log_data\n",
    "        log_data[\"registros_lidos\"] = df_read.count()\n",
    "\n",
    "        # Salvar o df_read na camada gold\n",
    "        df_read = save_table(df_read, new_file_name)\n",
    "\n",
    "        # df_read e nulo ou 0\n",
    "        if not df_read:\n",
    "            return\n",
    "        \n",
    "        # Conta quantos registros foram gravados\n",
    "        log_data[\"registros_gravados\"] = df_read.count()\n",
    "        \n",
    "        # Atualiza o log\n",
    "        if not update_log:\n",
    "            return\n",
    "        \n",
    "        print(f\"Arquivo {file} processado com sucesso.\")\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d86e73f5-50b1-4769-835a-93f71c40888c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50163fbc-6d09-4856-a29f-79f2c25f3cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume = \"download001\"\n",
    "camada = \"gold\"\n",
    "\n",
    "new_files_names = [\n",
    "    \"dim_categoria_produto\",\n",
    "    \"dim_cliente\",\n",
    "    \"dim_data\", \n",
    "    \"dim_localidade\",\n",
    "    \"dim_produto\",\n",
    "    \"fato_vendas\"\n",
    "] \n",
    "\n",
    "execute_gold_layer(volume, camada, new_files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda8a616-657a-4a6e-b52d-2b827655641d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Impressão dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a97866-8db9-463f-aeb0-e5270bbfd30d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "5": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770153041444}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 5
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "volume = \"download001\"\n",
    "camada = \"gold\"\n",
    "\n",
    "new_files_names = [\n",
    "    \"gold_dim_categoria_produto\",\n",
    "    \"gold_dim_cliente\",\n",
    "    \"gold_dim_data\", \n",
    "    \"gold_dim_localidade\",\n",
    "    \"gold_dim_produto\",\n",
    "    \"gold_fato_vendas\"\n",
    "] \n",
    "\n",
    "def see_tables(camada, new_file_names):\n",
    "    for file in new_files_names:\n",
    "        df = spark.read.format(\"delta\").load(f\"/Volumes/data_ex/{camada}/main/{file}\")\n",
    "        display(df)\n",
    "\n",
    "see_tables(\"gold\", new_files_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f0d47a-1d59-42e7-8e7c-1ee4cb4cd5d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Salvando como table para exportação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855dcef4-4a4e-40ee-991b-0b5359606937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpar Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8511083b-41c5-4798-bbe9-db4cb4b33c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19d15dc0-83b2-4869-b60c-58c60baff143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf382643-3a92-4eea-8aab-6079618f4a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"drop schema if exists data_ex.gold cascade\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold - artur",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
