{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dc2642-8f5b-4a50-a458-50b03cb09a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b864a4-b4ef-401e-b536-39f298ad8ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import gc\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType\n",
    "from datetime import datetime, date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da00847b-1a6e-4ac2-b935-2f6e4f0f2f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração da estrutura do Unity catalog\n",
    "\n",
    "* data_ex (catalog) \n",
    "    * metadados (schema)\n",
    "        * meta_bronze (table)\n",
    "        * meta_silver (table)\n",
    "        * meta_gold (table)\n",
    "    * lhdw (schema) \n",
    "        * versao001 (volumes)\n",
    "        * versao002 (volumes)\n",
    "        * versao003 (volumes)       \n",
    "    * bronze (schema)\n",
    "        * bronze_log_carga (table)\n",
    "        * versao003 (volume)\n",
    "            * bronze_dim_categoria_produto\n",
    "            * bronze_dim_cliente \n",
    "            * bronze_dim_data \n",
    "            * bronze_dim_localidade\n",
    "            * bronze_dim_produto \n",
    "            * bronze_fato_vendas  \n",
    "    * silver (schema)\n",
    "        * versao* (volume)\n",
    "            * silver_dim_categoria_produto\n",
    "            * silver_dim_cliente \n",
    "            * silver_dim_data \n",
    "            * silver_dim_localidade\n",
    "            * silver_dim_produto \n",
    "            * silver_fato_vendas\n",
    "    * gold  (schema)\n",
    "        * versao* (volume)\n",
    "            * dim_categoria_produto\n",
    "            * dim_cliente \n",
    "            * dim_data \n",
    "            * dim_localidade\n",
    "            * dim_produto \n",
    "            * fato_vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe8c4f2-4e87-49b2-9334-6d15912520ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"create catalog if not exists data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"create schema if not exists data_ex.metadados\")\n",
    "spark.sql(\"create schema if not exists data_ex.lhdw\")\n",
    "spark.sql(\"create schema if not exists data_ex.bronze\")\n",
    "spark.sql(\"create schema if not exists data_ex.silver\")\n",
    "spark.sql(\"create schema if not exists data_ex.gold\")\n",
    "\n",
    "# Criação das tabelas de metadados fixos, para controle de job \n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_bronze (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_silver (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists data_ex.metadados.meta_gold  (\n",
    "    id_job string\n",
    ") using delta \"\"\")\n",
    "\n",
    "# Abaixo são geradas uuids do tipo 4, a qual não requer argumentos e é gerada aleatoriamente\n",
    "id_job_bronze = str(uuid.uuid4())\n",
    "id_job_silver = str(uuid.uuid4())\n",
    "id_job_gold = str(uuid.uuid4())\n",
    "\n",
    "# Armazena a uuid gerada na respectiva camada\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_bronze values ('{id_job_bronze}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_silver values ('{id_job_silver}')\")\n",
    "spark.sql(f\"insert into data_ex.metadados.meta_gold values ('{id_job_gold}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df40a62b-459a-4c4f-9931-bd0ee30d6f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuração da sessão de trabalho bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d8a320e-9cb2-4d1d-95a2-6596dda83d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Otimização retirada do material disponibilizado do curso de Databricks\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96201fd3-8db9-4db0-b5e9-09674f9a7720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4765ed89-283e-4ea7-818d-2d24a54b8d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (volume || 0) download_dataset(csv_files, url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff77f553-0771-4741-9cb6-835c1e9b973f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset(csv_files, url, path):\n",
    "    # Tenta realizar o download dos arquivos\n",
    "    try :\n",
    "        # Variavel para armazenar o nome do novo volume\n",
    "        volume = \"\"\n",
    "        try:\n",
    "            # Conta quantos volumes já existem dentro desse do schema\n",
    "            numero = spark.sql(\"show volumes in data_ex.lhdw\").count()\n",
    "            # Cria o comando sql para criar um novo volume para a nova versao do download\n",
    "            volume = f\"download{numero+1:03d}\"\n",
    "            sql_command = f\"create volume if not exists data_ex.lhdw.{volume}\"\n",
    "            # Executa o comando sql acima\n",
    "            spark.sql(sql_command)\n",
    "        # lanca um execao caso houver um erro\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar o volume: {e}\")\n",
    "            return 0\n",
    "        for file in csv_files:\n",
    "            down_path = f\"{url+file}\"\n",
    "            up_path = f\"{path+volume}/{file}\"\n",
    "            urllib.request.urlretrieve(down_path, up_path)\n",
    "            print(f\"Csv {file} baixado com sucesso!\")\n",
    "    # Caso não consigua realizar o download, uma exceção é lançada\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {file} - {e}\")\n",
    "        return 0\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c884146-f343-4114-93ef-19d523d578f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## makeLogTable():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312e6806-f226-49e4-abe2-2906385f3e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_log_tabel(camada):\n",
    "    relacoes = {\n",
    "            \"bronze\" : \"bronze_log_carga\",\n",
    "            \"silver\" : \"silver_log_carga\",\n",
    "            \"gold\"   : \"gold_log_carga\"\n",
    "        }\n",
    "    n = 0\n",
    "    # Conta quantas tabelas existem\n",
    "    try:\n",
    "        n = spark.sql(\"show tables in data_ex.bronze\").count()\n",
    "    except:\n",
    "        print(\"ERRO ao ler a tabela de logs\")\n",
    "    # Cria a tabela se não existir \n",
    "    if n == 0:\n",
    "        try :\n",
    "            spark.sql(f\"\"\"create table if not exists data_ex.bronze.{relacoes[camada]} (\n",
    "                id_carga string,\n",
    "                id_job string,\n",
    "                nome_arquivo string,\n",
    "                fonte string,\n",
    "                camada string,\n",
    "                path_origem string,\n",
    "                path_destino string,\n",
    "                data_inicio timestamp,\n",
    "                data_fim timestamp,\n",
    "                duracao_ms string,\n",
    "                registros_lidos integer,\n",
    "                registros_gravados integer,\n",
    "                status string,\n",
    "                mensagem_erro string,\n",
    "                data_execusao date\n",
    "                ) using delta \"\"\")\n",
    "            print(\"gerougerou\")\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar a tabela de logs: {e}\")\n",
    "            return 0\n",
    "    # Erro, existe mais de uma tabela\n",
    "    elif n > 1:\n",
    "        print(f\"Erro na árvore de diretórios, exite mais de {n} tabelas de losgs na camada bronze\")\n",
    "        return 0\n",
    "    # A tabela ja existe\n",
    "    else:\n",
    "        print(\"Tabela de logs ja existes na camada bronze\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cf9113-bac9-4bdd-a126-96a463eff16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## generate_log(volume, camada, file, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c857c7ed-d313-4668-a9a5-c15dea315fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_log(volume, camada, file, new_name, schema_log):\n",
    "  camada_origem = \"lhdw\"\n",
    "  global log_data\n",
    "  \n",
    "  log_data.update({\n",
    "    \"id_carga\" : str(uuid.uuid4()),\n",
    "    \"id_job\"   : spark.sql(\"select * from data_ex.metadados.meta_bronze\").collect()[0][0],\n",
    "    \"nome_arquivo\" : new_name,\n",
    "    \"fonte\" : \"filesystem_local\",\n",
    "    \"camada\" : \"bronze\",\n",
    "    \"path_origem\" : f\"/Volumes/data_ex/bronze/{volume}/{file}\",\n",
    "    \"path_destino\" : f\"/Volumes/data_ex/silver/{volume}/{new_name}\",\n",
    "    \"data_inicio\" : spark.range(1).select(current_timestamp()).collect()[0][0],\n",
    "    \"status\" : \"Running\",\n",
    "    \"data_execusao\" : spark.range(1).select(current_date()).collect()[0][0]\n",
    "  })\n",
    "\n",
    "  try:\n",
    "    df = spark.createDataFrame([(\n",
    "      log_data[\"id_carga\"],\n",
    "      log[\"id_job\"],\n",
    "      log_data[\"nome_arquivo\"],\n",
    "      \"filesystem_local\",\n",
    "      camada,\n",
    "      str(f\"/Volumes/data_ex/bronze/{volume}/{file}\"), \n",
    "      str(f\"/Volumes/data_ex/silver/{volume}/{new_name}\"),\n",
    "      log_data[\"data_inicio\"],\n",
    "      None,\n",
    "      None,\n",
    "      None,\n",
    "      None,\n",
    "      \"Running\",\n",
    "      None,\n",
    "      log_data[\"data_execusao\"]\n",
    "    )], schema = schema_log)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_ex.bronze.bronze_log_carga\")\n",
    "    return 1\n",
    "  except Exception as e:\n",
    "    print(f\"Erro ao gerar o log: {e}\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4090c4-5c37-4349-a3cb-69cad9d51c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## log_update_leitura()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f85fa9a-3eb5-4388-9921-aa689572a00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def log_update_leitura(registros_lidos):\n",
    "    try:\n",
    "        global log_data\n",
    "\n",
    "        row = Row(\n",
    "            id_carga=log_data[\"id_carga\"],\n",
    "            registros_lidos=registros_lidos\n",
    "        )\n",
    "\n",
    "        df_update = spark.createDataFrame([row])\n",
    "        df_update.createOrReplaceTempView(\"log_leitura\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "            MERGE INTO data_ex.bronze.bronze_log_carga as tgt\n",
    "            USING log_leitura as src\n",
    "            ON tgt.id_carga = src.id_carga\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                tgt.registros_lidos = src.registros_lidos\n",
    "        \"\"\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao atualizar o log de leitura: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1be44f-0a96-4ff4-9852-b22b11e8346d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## update_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "13ed8fa1-6340-4763-a74f-caeeebe123bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_log():\n",
    "    try:\n",
    "        global log_data\n",
    "        log_data[\"status\"] = \"True\"   # Atualiza o status do log\n",
    "        data_fim = spark.range(1).select(current_timestamp()).collect()[0][0]\n",
    "        log_data[\"data_fim\"] = data_fim  \n",
    "        duracao_ms = (data_fim - log_data[7]).total_seconds() * 1000\n",
    "        log_data[\"duracao_ms\"] = duracao_ms\n",
    "        acao = store_log()\n",
    "        if not acao:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na funcao gerarLog(): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f3e669-a178-4f85-9b68-1343bb797030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## store_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "73852e15-67a1-4d5f-98af-f23d44f5e0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def store_log():\n",
    "    try:\n",
    "        global log_data\n",
    "\n",
    "        row = Row(\n",
    "            id_carga=log_data[\"id_carga\"],\n",
    "            id_job=log_data[\"id_job\"],\n",
    "            nome_arquivo=log_data[\"nome_arquivo\"],\n",
    "            fonte=log_data[\"fonte\"],\n",
    "            camada=log_data[\"camada\"],\n",
    "            path_origem=log_data[\"path_origem\"],\n",
    "            path_destino=log_data[\"path_destino\"],\n",
    "            data_inicio=log_data[\"data_inicio\"],\n",
    "            data_fim=log_data[\"data_fim\"],\n",
    "            duracao_ms=log_data[\"duracao_ms\"],\n",
    "            registros_lidos=log_data[\"registros_lidos\"],\n",
    "            registros_gravados=log_data[\"registros_gravados\"],\n",
    "            status=log_data[\"status\"],\n",
    "            mensagem_erro=log_data[\"mensagem_erro\"],\n",
    "            data_carga=log_data[\"data_carga\"]\n",
    "        )\n",
    "\n",
    "        df_log = spark.createDataFrame([row])\n",
    "        df_log.createOrReplaceTempView(\"log_final\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "            MERGE INTO data_ex.bronze.bronze_log_carga AS tgt\n",
    "            USING log_final AS src\n",
    "            ON tgt.id_carga = src.id_carga\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                tgt.id_job = src.id_job,\n",
    "                tgt.nome_arquivo = src.nome_arquivo,\n",
    "                tgt.fonte = src.fonte,\n",
    "                tgt.camada = src.camada,\n",
    "                tgt.path_origem = src.path_origem,\n",
    "                tgt.path_destino = src.path_destino,\n",
    "                tgt.data_inicio = src.data_inicio,\n",
    "                tgt.data_fim = src.data_fim,\n",
    "                tgt.duracao_ms = src.duracao_ms,\n",
    "                tgt.registros_lidos = src.registros_lidos,\n",
    "                tgt.registros_gravados = src.registros_gravados,\n",
    "                tgt.status = src.status,\n",
    "                tgt.mensagem_erro = src.mensagem_erro,\n",
    "                tgt.data_carga = src.data_carga\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                id_carga,\n",
    "                id_job,\n",
    "                nome_arquivo,\n",
    "                fonte,\n",
    "                camada,\n",
    "                path_origem,\n",
    "                path_destino,\n",
    "                data_inicio,\n",
    "                data_fim,\n",
    "                duracao_ms,\n",
    "                registros_lidos,\n",
    "                registros_gravados,\n",
    "                status,\n",
    "                mensagem_erro,\n",
    "                data_carga\n",
    "            )\n",
    "            VALUES (\n",
    "                src.id_carga,\n",
    "                src.id_job,\n",
    "                src.nome_arquivo,\n",
    "                src.fonte,\n",
    "                src.camada,\n",
    "                src.path_origem,\n",
    "                src.path_destino,\n",
    "                src.data_inicio,\n",
    "                src.data_fim,\n",
    "                src.duracao_ms,\n",
    "                src.registros_lidos,\n",
    "                src.registros_gravados,\n",
    "                src.status,\n",
    "                src.mensagem_erro,\n",
    "                src.data_carga\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        return 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gravar o log (MERGE): {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4eb7532-2613-4401-861d-18d999a95cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## read_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc4949a-291e-4403-84b9-fdff6c097b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_dim(path, file, schema):\n",
    "    try:\n",
    "        df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .schema(schema)\\\n",
    "            .csv(f\"{path}/{file}\")\n",
    "        print(\"ler dim\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"ler dim\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa05681c-4421-4136-8bfb-e2d9be77ee86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## save_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65e72c4-ec64-4207-8ac5-9a98ba5bfdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_dim(df_read, dime, camada):\n",
    "    try:\n",
    "        df_fato = df_read.write.format(\"delta\").mode(\"overwrite\").save(f\"{camada}/{dime}\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fa68ae-531a-4263-b8b9-5058a97589bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## read_fato()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d4f3fd-17e6-4dc3-85a3-633ccc9dbbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_fato(path, files, schema, n):\n",
    "    print(\"entrou na read_fato()\")\n",
    "    try:\n",
    "        vendas_df = [0]\n",
    "        # Aplica um loop para ler todos os arquivos vendas\n",
    "        for i in range(0, n):\n",
    "            try:\n",
    "                df = spark.read.option(\"header\", \"true\")\\\n",
    "                    .schema(schema_vendas)\\\n",
    "                    .csv(f\"{path}/{files[i]}\")\n",
    "                vendas_df[0] = vendas_df[0] + df.count()\n",
    "                vendas_df.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler o arquivo {path}/{files[i]}: {e}\")\n",
    "        return vendas_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo {files[i]}: {e}\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6b381a-ab39-4bb3-9892-6b7a9df46a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save_fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae35212e-a296-4b93-8308-e70f8c59db54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_fato(df, fato, n, camada):\n",
    "    #Loop para criar a tabela de fato vendas\n",
    "    for i in range(0, n):\n",
    "        try:\n",
    "            df[i].write.format(\"delta\").mode(\"append\").save(f\"{camada}/{fato}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe vendas: {e}\")\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d19ba8-e768-4644-b40f-1bc68118407a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funcao de Comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85b20c9-4e5e-4e8d-8015-efeee02a6179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bronze_work(csv_files, url, lhdw_path, schemas, dim_name, fato_name, camada, n, schema_log):\n",
    "  \n",
    "  # Faz o download do Dataset, cria um novo volume para armazenar os arquivos contidos\n",
    "  volume_salvo = download_dataset(csv_files, url, lhdw_path)\n",
    "  if not volume_salvo:\n",
    "    return\n",
    "\n",
    "  # Garante a existencia da tabelas de logs de carga da camada bronze\n",
    "  if not make_log_tabel(camada):\n",
    "    return\n",
    "      \n",
    "  # Definição dos diretorios de leitura\n",
    "  lhdw_path = f\"{lhdw_path}/{volume_salvo}\"\n",
    "\n",
    "  # Criação do volume no schema bronze\n",
    "  spark.sql(f\"create volume if not exists data_ex.bronze.{volume_salvo}\")\n",
    "\n",
    "  # Definição do path bronze\n",
    "  bronze_path = f\"/Volumes/data_ex/bronze/{volume_salvo}\"\n",
    "\n",
    "  bronze_work_dim(csv_files, schemas, dim_name, camada, n, schema_log, lhdw_path, volume_salvo)\n",
    "\n",
    "  bronze_work_fato(csv_files, schemas, camada, fato_name, volume_salvo, n, lhdw_path, volume_salvo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23eb728e-bcfd-40a0-8269-f81a65d43e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_data = {}\n",
    "def bronze_work_dim(csv_files, schemas, dime, camada, n, schema_log, lhdw_path, volume):\n",
    "    print(\"dim\")\n",
    "  # Lista com o nome dos arquivos csv dimensão a serem processados \n",
    "    files = csv_files[:-n]\n",
    "    \n",
    "  # indice para registrar o nome da dimenssao a ser salvada\n",
    "    for i, (file, schema, new_name) in enumerate(zip(files, schemas, dime)):\n",
    "        print(f\"{i},{file}\")\n",
    "        # Tenta ler os arquivos csv em lhdw_path\n",
    "            \n",
    "        # Inicia o registo de logs\n",
    "        if not  generate_log(volume, camada, file, new_name, schema_log):\n",
    "            break\n",
    "        print(\"loglog\")\n",
    "        # Se schema não for vendas, aplica o schema correspondente a file\n",
    "        if schema != schema_vendas:\n",
    "        \n",
    "            # Tenta chamar a funcao para ler um arquivo de nao vendas\n",
    "            df_read = read_dim(lhdw_path, file, schema)\n",
    "            if not df_read:\n",
    "                printf(f\"Erro ao ler o arquivo {file}\")\n",
    "                break\n",
    "            \n",
    "            # Lê o arquivo csv\n",
    "            global log_data\n",
    "            registros_lidos = df_read.count()\n",
    "            log_data[\"registros_lidos\"] = registros_lidos\n",
    "            if not log_update_leitura(registros_lidos):\n",
    "                print(f\"Erro ao gravar a quantidade de dados lidos\")\n",
    "                break\n",
    "            \n",
    "            # Salva\n",
    "            save = save_dim(df_read, dime[i], camada)\n",
    "            if not save:\n",
    "                printf(f\"Erro ao salvar o dataframe \")\n",
    "                break\n",
    "            \n",
    "            # Conta quantos registros foram salvos\n",
    "            registros_gravados = spark.read.format(\"delta\").load(f\"{camada}/{dime[i]}\").count()\n",
    "            log_data[\"registros_gravados\"] = registros_gravados\n",
    "\n",
    "            \n",
    "            acao = update_log()\n",
    "            if not acao:\n",
    "                printf(\"ERRO ao gravar o log\")\n",
    "                break\n",
    "            \n",
    "            print(f\"DataFrame salvo com sucesso em {camada}/{dime[i]}\")   \n",
    "            \n",
    "       \n",
    "            \n",
    "                \n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f08f1d71-62b0-4f71-a786-d587fdc51af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bronze_work_fato(csv_files, schemas, camada, fato_name, volume_salvo, n, lhdw_path, volume):\n",
    "    global log_data\n",
    "    print(\"fato\")\n",
    "    if not  generate_log(volume, camada, \"vendas\", \"bronze_dim_vendas\", schema_log):\n",
    "        printf(\"ERRO ao gerar log\")\n",
    "        return\n",
    "    \n",
    "    # Chama a função de leitura\n",
    "    df_read = read_fato(lhdw_path, csv_files[-4:] ,schema_vendas, n)\n",
    "\n",
    "    if not df_read:\n",
    "        print(f\"Erro ao ler os arquivos de vendas\")\n",
    "        return\n",
    "    \n",
    "    registros_lidos = df_read[0]\n",
    "    log_data[\"registros_lidos\"] = registros_lidos\n",
    "    df_read.pop(0)\n",
    "\n",
    "    if not log_update_leitura(registros_lidos):\n",
    "        print(f\"Erro ao gravar a quantidade de dados lidos em fatos\")\n",
    "        return\n",
    "\n",
    "    # Salvar\n",
    "    if not save_fato(df_read, fato_name, n, camada):\n",
    "        printf(f\"Erro ao chamar a função save_fato() : {e}\")\n",
    "        return\n",
    "    \n",
    "    # Salvamento concluído, armazenando o log respectivo\n",
    "    registros_gravados = spark.read.format(\"delta\").load(f\"{camada}/{fato_name}\").count()\n",
    "    log_data[\"registros_gravados\"] = registros_gravados\n",
    "\n",
    "    if not update_log():\n",
    "        return\n",
    "    \n",
    "    print(f\"DataFrame salvo com sucesso em {camada}/{fato_name}\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a940025-cc67-4e66-94ec-c08991c398d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dee3ce06-9c64-4ee5-828e-6ce2253e6fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista dos nomes dos schemas em ordem de execução\n",
    "schemas = []\n",
    "\n",
    "# Definição manual do schemas a serem aplicados nos arquivos csv (futuros parquets)\n",
    "schema_categoria = StructType([\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True),\n",
    "])\n",
    "schemas.append(schema_categoria)\n",
    "\n",
    "schema_cliente = StructType([\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"nome_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_cliente)\n",
    "\n",
    "schema_data = StructType([\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"data\", DateType(), True),\n",
    "    StructField(\"ano\", IntegerType(), True),\n",
    "    StructField(\"mes\", IntegerType(), True),\n",
    "    StructField(\"dia\", IntegerType(), True),\n",
    "    StructField(\"dia_semana\", StringType(), True), \n",
    "    StructField(\"final_de_semana\", BooleanType(), True)\n",
    "])\n",
    "schemas.append(schema_data)\n",
    "\n",
    "schema_localidade = StructType([\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    ])  \n",
    "schemas.append(schema_localidade)\n",
    "\n",
    "schema_produto = StructType([\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_produto)\n",
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"venda_id\", LongType(), True),\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"quantidade\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True)\n",
    "])\n",
    "schemas.append(schema_vendas)\n",
    "\n",
    "# schema dos logs\n",
    "\n",
    "schema_log = StructType([\n",
    "    StructField(\"id_carga\", StringType(), False),         \n",
    "    StructField(\"id_job\", StringType(), False),           \n",
    "    StructField(\"nome_arquivo\", StringType(), True),\n",
    "    StructField(\"fonte\", StringType(), True),\n",
    "    StructField(\"camada\", StringType(), False),\n",
    "    StructField(\"path_destino\", StringType(), True),\n",
    "    StructField(\"data_inicio\", TimestampType(), False),    \n",
    "    StructField(\"data_fim\", TimestampType(), True),       \n",
    "    StructField(\"duracao_ms\", LongType(), True),\n",
    "    StructField(\"registros_lidos\", LongType(), True),\n",
    "    StructField(\"registros_gravados\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"mensagem_erro\", StringType(), True),\n",
    "    StructField(\"data_execucao\", DateType(), False)        \n",
    "])\n",
    "\n",
    "\n",
    "# Lista de .csv a serem lidos\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Reposotório do dataset para o desafio\n",
    "url = \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\"\n",
    "\n",
    "# Path para o schema de armazenamento, sem o volume\n",
    "lhdw_path = \"/Volumes/data_ex/lhdw/\"\n",
    "\n",
    "# Lista de nomes das dimenssoes bronze\n",
    "dim_name = [\n",
    "  \"bronze_dim_categoria_produto\",\n",
    "  \"bronze_dim_cliente\",\n",
    "  \"bronze_dim_data\", \n",
    "  \"bronze_dim_localidade\",\n",
    "  \"bronze_dim_produto\", \n",
    "]\n",
    "\n",
    "# Nome da tabela fato\n",
    "fato_name = \"bronze_fato_vendas\"\n",
    "\n",
    "# Numero de csv que fazem parte da bronze fato\n",
    "n_fatos = 4\n",
    "\n",
    "camada = \"bronze\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f165cc97-2e41-4338-a63c-64bae4d9ca0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_work(csv_files, url, lhdw_path, schemas, dim_name, fato_name, camada, n_fatos, schema_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f8a91f-9575-4859-af10-4ed6e3d88076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW VOLUMES IN data_ex.desafio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5ff75e-6139-4bb8-a9b8-53e0be29bdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP schema data_ex.desafio;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e77f7b7-5168-44a8-b002-f233acb1e291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = f\"/Volumes/data_ex/bronze/download001/bronze_dim_categoria_produto\"\n",
    "df = spark.read.format(\"parquet\").load(bronze_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212c0853-5528-4aa8-81d8-0289ed0f62ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/Volumes/data_ex/lhdw/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5301856000034183,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronzeV2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
