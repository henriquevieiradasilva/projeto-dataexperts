{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6b66b5-6346-4a61-a6bb-26e5229b10ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuração final da estrutura de diretórios\n",
    "\n",
    "* data_ex (catalog) \n",
    "    * desafio (schema)\n",
    "        * lhdw (volume)\n",
    "        * bronze (volume)\n",
    "            * bronze_dim_categoria_produto\n",
    "            * bronze_dim_cliente \n",
    "            * bronze_dim_data \n",
    "            * bronze_dim_localidade\n",
    "            * bronze_dim_produto \n",
    "            * bronze_fato_vendas  \n",
    "        * silver (volume)\n",
    "            * silver_dim_categoria_produto\n",
    "            * silver_dim_cliente \n",
    "            * silver_dim_data \n",
    "            * silver_dim_localidade\n",
    "            * silver_dim_produto \n",
    "            * silver_fato_vendas\n",
    "        * gold  (volume)\n",
    "            * dim_categoria_produto\n",
    "            * dim_cliente \n",
    "            * dim_data \n",
    "            * dim_localidade\n",
    "            * dim_produto \n",
    "            * fato_vendas\n",
    "\n",
    "Tornar o escrito a cima em um diagrama para ser um dos anexos ao storytelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645a6909-75a7-4de7-a0b3-b04a7ac507ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"create catalog if not exists data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"create schema if not exists data_ex.desafio\")\n",
    "\n",
    "# Criação dos volumes\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.lhdw\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.bronze\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.silver\")\n",
    "spark.sql(\"create volume if not exists data_ex.desafio.gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c4cf92-ad08-4b35-b1bf-11f1bfe2f7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"drop volume if  exists data_ex.desafio.lhdw\")\n",
    "# spark.sql(\"drop volume if  exists data_ex.desafio.bronze\")\n",
    "# spark.sql(\"drop volume if  exists data_ex.desafio.silver\")\n",
    "# spark.sql(\"drop volume if  exists data_ex.desafio.gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2127ba-495c-4f5b-a93b-0abb97393980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Importação pelo Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca79ccab-b54b-4fa0-b2ac-74170697df69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a888e6b1-f8c9-4dc4-9ae3-67da3ec6c184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Função para baixar os arquivos\n",
    "def download_dataset(csv_files, url, lhdw_path):\n",
    "    # Tenta realizar o download dos arquivos\n",
    "    try :\n",
    "        for file in csv_files:\n",
    "            # url+file corresponde a url de download\n",
    "            # lhdw_path+file corresponde ao caminho onde o arquivo será salvo(lhdw_path) e, nome o formato do arquivo(file)\n",
    "            urllib.request.urlretrieve(url+file, lhdw_path+file)\n",
    "            print(f\"Csv {file} baixado com sucesso!\")\n",
    "    # Caso não consigua realizar o download, uma exceção é lançada\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {file} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec6e163-cc28-4882-9b7d-f11f09304eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilizaçao da função de download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378baa36-81fe-4034-be13-8b17aa6662b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de .csv\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Reposotório do dataset para o desafio\n",
    "url = \"https://raw.githubusercontent.com/andrerosa1977/dataexperts2026/main/\"\n",
    "lhdw_path = \"/Volumes/data_ex/desafio/lhdw/\"\n",
    "\n",
    "download_dataset(csv_files, url, lhdw_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cba06a-65ce-4cd7-a220-4d6ecced20fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83da5fa-bf55-4a4f-912c-9511344f507c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do servidor para trabalho no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f314e448-b30c-4b6a-9e40-1bc15c2a95bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f210f36-19a1-4e08-b5f2-95d047b606c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trabalhos no Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8329c678-272a-46e5-aa4c-bd382980c52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Função de Leitura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8431d0-aab7-4b64-b34a-fee28e8c78ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Variáveis para leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39fc985-77ff-4719-adb4-8944b0dbe401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas/funções necessárias\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, TimestampType, LongType, DateType, TimestampType, BooleanType, DoubleType\n",
    "\n",
    "# Definição dos diretorios de trabalho\n",
    "lhdw_path = \"/Volumes/data_ex/desafio/lhdw\"\n",
    "bronze_path = \"/Volumes/data_ex/desafio/bronze\"\n",
    "\n",
    "# Arquivos csv a serem trabalhados\n",
    "csv_files = [\n",
    "    \"categoria_produto.csv\",\n",
    "    \"cliente.csv\",\n",
    "    \"data.csv\",\n",
    "    \"localidade.csv\",\n",
    "    \"produto.csv\",\n",
    "    \"vendas_part1.csv\",\n",
    "    \"vendas_part2.csv\",\n",
    "    \"vendas_part3.csv\",\n",
    "    \"vendas_part4.csv\"\n",
    "]\n",
    "\n",
    "# Lista dos nomes dos schemas em ordem de execução\n",
    "schemas = []\n",
    "\n",
    "\n",
    "# Definição manual do schemas a serem aplicados nos arquivos csv (futuros parquets)\n",
    "schema_categoria = StructType([\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True),\n",
    "])\n",
    "schemas.append(schema_categoria)\n",
    "\n",
    "schema_cliente = StructType([\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"nome_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_cliente)\n",
    "\n",
    "schema_data = StructType([\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"data\", DateType(), True),\n",
    "    StructField(\"ano\", IntegerType(), True),\n",
    "    StructField(\"mes\", IntegerType(), True),\n",
    "    StructField(\"dia\", IntegerType(), True),\n",
    "    StructField(\"dia_semana\", StringType(), True), \n",
    "    StructField(\"final_de_semana\", BooleanType(), True)\n",
    "])\n",
    "schemas.append(schema_data)\n",
    "\n",
    "schema_localidade = StructType([\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    ])  \n",
    "schemas.append(schema_localidade)\n",
    "\n",
    "schema_produto = StructType([\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"categoria_nome\", StringType(), True)\n",
    "])\n",
    "schemas.append(schema_produto)\n",
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"venda_id\", LongType(), True),\n",
    "    StructField(\"cliente_id\", LongType(), True),\n",
    "    StructField(\"produto_id\", LongType(), True),\n",
    "    StructField(\"data_id\", LongType(), True),\n",
    "    StructField(\"categoria_id\", LongType(), True),\n",
    "    StructField(\"localidade_id\", LongType(), True),\n",
    "    StructField(\"quantidade\", LongType(), True),\n",
    "    StructField(\"preco_lista\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True)\n",
    "])\n",
    "schemas.append(schema_vendas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2150e576-d26b-4429-a8da-0ab472fe8090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando uma lista para armazenar os Sparks DataFrames\n",
    "spark_df = []\n",
    "\n",
    "# Criação do Spark DataFrame, leitura dos csv, aplicação dos schemas e escrita em parquet \n",
    "def bronze_read(csv_files, schemas):\n",
    "    for file, schema in zip(csv_files, schemas):\n",
    "        # Tenta ler os arquivos csv em lhdw_path\n",
    "        try:\n",
    "            # Se schema não for vendas, aplica o schema correspondente a file\n",
    "            if schema != schema_vendas:\n",
    "                df = spark.read\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(schema)\\\n",
    "                    .csv(f\"{lhdw_path}/{file}\")\n",
    "                print(f\"Leitura do arquivo {file} bem sucedida.\")\n",
    "                spark_df.append(df)\n",
    "\n",
    "            # Se for vendas, deve repetir a leirura do arquivo n vezes\n",
    "            else:\n",
    "                # Tenta ler o caminho no qual vendas esta\n",
    "                try:\n",
    "                    paths = dbutils.fs.ls(lhdw_path)\n",
    "                    # Conta o numero csv vendas existentes\n",
    "                    n = len([n for n in paths if n.name.startswith(\"vendas\")])        \n",
    "                    # Aplica um loop para ler todos os arquivos vendas\n",
    "                    for i in range(1, n+1):\n",
    "                        try:\n",
    "                            df = spark.read.option(\"header\", \"true\")\\\n",
    "                                            .schema(schema_vendas)\\\n",
    "                                            .csv(f\"{lhdw_path}/vendas_part{i}.csv\")\n",
    "                            print(f\"Leitura do arquivo vendas_part{i}.csv bem sucedida.\")\n",
    "                           \n",
    "                        except Exception as e:\n",
    "                            print(f\"Erro ao ler o arquivo {lhdw_path}/vendas_part{i}.csv: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(\"ERRO\")\n",
    "                    print(f\"Erro ao ler o caminho no qual no qual vendas esta {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "\n",
    "bronze_read(csv_files, schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691f7456-a8b0-470d-8835-3b96f64beab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Registro de Logs de Carga\n",
    "* id_carga (string (uuid))\n",
    "* id_job (string (uuid))\n",
    "* nome_arquivo (string)\n",
    "* fonte (string)\n",
    "* camada (string)\n",
    "* path_destino (string)\n",
    "* data_inicio (utc_timestamp)\n",
    "* data_fim (utc_timestamp)\n",
    "* duracao_ms (bigint)\n",
    "* registros_lidos (bigint)\n",
    "* registros_gravados (bigint)\n",
    "* status (string)\n",
    "* mensagem_erro (string)\n",
    "* data_execucao (date(yyyy-MM-dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7a6012-9ade-42f5-9210-cff3be13ce9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff77e76-9a2c-4406-bc54-cd4fcd7cfc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvar no Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9f228c-4a0e-41b9-abe4-3002f44e886c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para salvar os arquivos na camada bronze\n",
    "def save(spark_df, path, file_names):\n",
    "    # Variaveis auxiliares\n",
    "    n_dims = len(file_names) - 1     \n",
    "    m_vendas = 0         \n",
    "    vendas_name = file_names[-1]              \n",
    "    \n",
    "    # Loop para salvar os arquivos na camada bronze com execao dos dfs de vendas (relacionamento 1:1)\n",
    "    for i in range(n_dims):\n",
    "        try:\n",
    "            spark_df[i].write.mode(\"append\").parquet(f\"{path}/{file_names[i]}\")\n",
    "            m_vendas = 1\n",
    "            print(f\"DataFrame salvo com sucesso em {path}/{file_names[i]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe: {e}\")\n",
    "\n",
    "    #Loop para salvar todas as tabelas de vendas em um unico arquivo (relacionamento 1:n)\n",
    "    for i in range(m_vendas, len(spark_df)):\n",
    "        try:\n",
    "            spark_df[i].write.mode(\"append\").parquet(f\"{path}/bronze_fato_vendas\")\n",
    "            print(f\"DataFrame salvo com sucesso em {path}/bronze_fato_vendas\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar o dataframe vendas: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410092a0-7455-49e8-adde-47b85cc49ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição do nomes das dimensões a serem salvas\n",
    "file_names = [\n",
    "  \"bronze_dim_categoria_produto\",\n",
    "  \"bronze_dim_cliente\",\n",
    "  \"bronze_dim_data\", \n",
    "  \"bronze_dim_localidade\",\n",
    "  \"bronze_dim_produto\", \n",
    "  \"bronze_fato_vendas\",\n",
    "]\n",
    "\n",
    "# Chamada da função de salvamento\n",
    "save(spark_df, bronze_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e12545-bec2-4a5d-8e1e-ea6efb48c24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de682b3-9be7-4aa9-876e-7e920134be33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limpar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749206e6-356b-4ec1-b63c-f05ba0a5f7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
