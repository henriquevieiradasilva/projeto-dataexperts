{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80024867-4e20-41ee-8d8e-c02167642565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 4 - PROCESSAMENTO PARA CAMADA SILVER**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada silver, filtrando e limpando os dados. \n",
    "\n",
    "> (Esse protótipo usa de referência fazer a desnormalização apenas na camada gold, mas caso necessário pode ser feito já nessa camada silver também)\n",
    "\n",
    "<br>\n",
    "\n",
    "*`Esse é um modelo genérico, coloque as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO IMPORTANTE: No Databricks Free Edition trabalha-se com catálogos, que é a maneira de na prática aplicar um Data Lakehouse***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7c3e41-d88f-4866-b14e-ea897134430c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f95bc9-13a8-40a3-b5f0-2abb3dbdb389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**\n",
    "\n",
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ebd850-639e-4de8-8ebf-c6ccb8031e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Load Data Bronze\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d92e830-55eb-435a-b007-7b1f15206404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Definindo Origens e Destinos**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse ` <br>\n",
    "`--> nome_camada_origem --> nome da camada de origem dos dados` <br>\n",
    "`--> nome_volume_origem --> nome do volume de origem dos dados dentro da camada` <br>\n",
    "`--> nome_camada_silver --> nome da camada de destino dos dados` <br>\n",
    "`--> nome_volume_silver --> nome do volume de destino dos dados dentro da camada` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684a028b-ea7e-491a-aad3-7e1fb81eb732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"lhdw\"\n",
    "nome_camada_origem = \"bronze\"\n",
    "nome_volume_origem = \"vendas\"\n",
    "nome_camada_silver = \"silver\"\n",
    "nome_volume_silver = \"vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdeac567-3f89-4d03-8177-b931c118aeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c37e7a-de2c-4c56-8258-9bb5e2787f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "silver_destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_silver}/{nome_volume_silver}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe8ebd2-35da-4f79-8a46-beb5c1e6552a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume de destino caso ele ainda não exista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6142f41f-dcfd-4591-811e-eb3077271b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_silver}.{nome_volume_silver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fb1c3a-963a-4f3e-9a89-ad6d13dc63e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Leitura dos Dados**\n",
    "\n",
    "\n",
    "O código a seguir lê todos os dados e colocar em um Data Frame para que seja possível manipular os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e02781-d836-49a6-bcf9-e5d9dd160fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_dados = spark.read.format(\"parquet\").load(origem_dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3f7b7e-b4ad-4857-9eb1-59bcfd5c70af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 5 - **Limpeza dos Dados**\n",
    "\n",
    "A limpeza de dados é um processo crucial para garantir a **qualidade dos dados**. Isso envolve a remoção de **dados duplicados ou incorretos**, a **padronização de formatos e valores de dados** e o **enriquecimento de dados** com informações adicionais. Tudo isso para **garantir** que os **dados** sejam **precisos** e **confiáveis**.\n",
    "\n",
    "Cada coluna deve ser analisada para encontrar possíveis inconformidades que podem ser encontradas e resolvidas nessa estapa, entre elas estão:\n",
    "- Remoção de dados duplicados \n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde3dc4e-325c-4d95-b713-2c5da826d46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dataframe_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a0338d-0919-462a-bca4-e378530668ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_silver = (\n",
    "    dataframe_dados.withColumn(\"Data\", to_date(col(\"Data\"), \"yyyy-MM-dd\")) \n",
    "        .withColumn(\"Email\", lower(expr(\"regexp_replace(split(EmailNome, ':')[0], '[()]', '')\"))) \n",
    "        .withColumn(\"Nome\", expr(\"split(split(EmailNome, ':')[1], ', ')\")) \n",
    "        .withColumn(\"Nome\", expr(\"concat(Nome[1], ' ', Nome[0])\")) \n",
    "        .withColumn(\"Cidade\", expr(\"split(Cidade, ',')[0]\")) \n",
    "        .withColumn(\"PrecoUnitario\", format_number(col(\"PrecoUnitario\"), 2)) \n",
    "        .withColumn(\"CustoUnitario\", format_number(col(\"CustoUnitario\"), 2)) \n",
    "        .withColumn(\"TotalVendas\", format_number(col(\"PrecoUnitario\").cast(\"double\") * col(\"Unidades\").cast(\"double\"),2))\n",
    "        .drop(\"EmailNome\")\n",
    "        .drop(\"IdCampanha\")   \n",
    ")\n",
    "                     \n",
    "display(dataframe_silver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe22114-d210-4e25-bd6f-c23fddcb0adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte - ****\n",
    "\n",
    "Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d8b70b-9bfa-472c-a99b-bd6f4dcaf843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17fff22-b80a-46a2-be41-13619a216ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir é uma das maneiras simples de se mostrar se deu certo ou não a importação do arquivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e336b78-5745-4d4c-81e8-81047aa49565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "\"\"\"\n",
    " #\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 3 - Processamento para Camada Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
