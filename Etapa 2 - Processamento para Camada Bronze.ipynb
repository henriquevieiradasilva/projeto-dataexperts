{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672cd9a8-0a64-4900-9601-f8f68cc25a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 3 - PROCESSAMENTO PARA CAMADA BRONZE**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa etapa será responsável por mover os dados para a camada bronze, colocando em um formato padrão mas sem mais nenhum refinamento, ainda mantendo os dados brutos.\n",
    "\n",
    "*`Esse é um modelo genérico, coloque as informações necessárias nos trechos que estão destacados em vermelho assim como esse, seguindo o padrão snake_case.`*\n",
    "\n",
    "<br> \n",
    "\n",
    "***AVISO IMPORTANTE: No Databricks Free Edition trabalha-se com catálogos, que é a maneira de na prática aplicar um Data Lakehouse***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Importação das Bibliotecas Necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5189e80d-cbdc-4c12-b5af-5d412e62fe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, \n",
    "    year, \n",
    "    month, \n",
    "    regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField,\n",
    "    IntegerType, \n",
    "    StringType, \n",
    "    DoubleType, \n",
    "    DateType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b38c156-a91f-4ec5-936b-324b57f4df20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 2 - **Otimizar a Sessão com configurações Personalizadas**\n",
    "\n",
    "Aqui o será configurado algumas propriedades para que o desempenho da sessão seja mais otimizado \n",
    "- Define tamanho fixo de partições para o shuffle para melhorar o paralelismo (usar ***número de partições = número de núclos de CPU * 2 ou 3*** para encontrar melhor cenário possível)\n",
    "- Define o tamanho máximo de partições para evitar muitos arquivos pequenos\n",
    "- Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita\n",
    "- Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b93818-9cf5-4998-9af0-b1190f26c82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Load Data Bronze\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4ecd4f-680a-4c37-afac-19f694c45665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 3 - **Definindo Origens e Destinos**\n",
    "\n",
    "`Insira nas variáveis:` <br>\n",
    "`--> nome_datalakehouse --> nome do Data Lakehouse ` <br>\n",
    "`--> nome_camada_origem --> nome da camada de origem dos dados` <br>\n",
    "`--> nome_volume_origem --> nome do volume de origem dos dados dentro da camada` <br>\n",
    "`--> nome_camada_bronze --> nome da camada de destino dos dados` <br>\n",
    "`--> nome_volume_bronze --> nome do volume de destino dos dados dentro da camada` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28a2622-8e18-4bb2-999c-6e5b18c93766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_datalakehouse = \"lhdw\"\n",
    "nome_camada_origem = \"source\"\n",
    "nome_volume_origem = \"vendas\"\n",
    "nome_camada_bronze = \"bronze\"\n",
    "nome_volume_bronze = \"vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672b818d-7a41-4de0-b660-4e2b3c31919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir armazena em variáveis os caminhos já prontos de origem e de destino dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6524bea1-6541-4b2a-86f2-fdd233857669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origem_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_origem}/{nome_volume_origem}/\"\n",
    "bronze_destino_dados = f\"/Volumes/{nome_datalakehouse}/{nome_camada_bronze}/{nome_volume_bronze}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd534185-2e50-4d4e-9765-f9eab33b9742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir cria o volume de destino caso ele ainda não exista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad938f9-7d50-4430-b4a0-36e419c4161c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE VOLUME IF NOT EXISTS {nome_datalakehouse}.{nome_camada_bronze}.{nome_volume_bronze}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7add9c-0e8a-4eaf-bf9f-454df4bd5177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 4 - **Leitura dos Dados**\n",
    "\n",
    "Para que seja possível manipular os dados, antes precisamos ler todos eles já com uma estrutura pré-definida.\n",
    "\n",
    "O código a seguir cria essa estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76db3fa0-8b17-4855-b1ad-2181380db5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "estrutura_schema_dados = StructType([\n",
    "    StructField(\"IDProduto\", IntegerType(), True),\n",
    "    StructField(\"Data\", DateType(), True),\n",
    "    StructField(\"IDCliente\", IntegerType(), True),\n",
    "    StructField(\"IDCampanha\", IntegerType(), True),\n",
    "    StructField(\"Unidades\", IntegerType(), True),\n",
    "    StructField(\"Produto\", StringType(), True),\n",
    "    StructField(\"Categoria\", StringType(), True),\n",
    "    StructField(\"Segmento\", StringType(), True),\n",
    "    StructField(\"IDFabricante\", IntegerType(), True),\n",
    "    StructField(\"Fabricante\", StringType(), True),\n",
    "    StructField(\"CustoUnitario\", DoubleType(), True),\n",
    "    StructField(\"PrecoUnitario\", DoubleType(), True),\n",
    "    StructField(\"CodigoPostal\", StringType(), True),\n",
    "    StructField(\"EmailNome\", StringType(), True),\n",
    "    StructField(\"Cidade\", StringType(), True),\n",
    "    StructField(\"Estado\", StringType(), True),\n",
    "    StructField(\"Regiao\", StringType(), True),\n",
    "    StructField(\"Distrito\", StringType(), True),\n",
    "    StructField(\"Pais\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f00d122-51be-497c-a94f-81b1a955dd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código a seguir faz a leitura dos dados, adicionando a coluna nome do arquivo durante a leitura (usando de base a estrutura feita no código anterior) e armarzenando o resultado desse processo em um data frame para que seja possível manipular esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad0d4aa-c1f4-4caf-ae71-0abb500a9d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_dados = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(estrutura_schema_dados)\n",
    "        .csv(origem_dados)\n",
    "        .withColumn(\n",
    "            \"filename\",\n",
    "            regexp_extract(col('_metadata.file_path'), \"([^/]+)$\", 0)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9de94b-2481-4482-ba2c-18069d1aadde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 5 - **Salvando os Dados no Formato e Local Corretos**\n",
    "\n",
    "Por fim, precisamos criar o parquet com os dados que importamos e com a estrutura que construimos anteriormente e mover para camada bronze.\n",
    "\n",
    "O código a seguir faz essa compressão e envio para o caminho certo (já particionando em ano e mês para ser mais otimizado quando precisarmos utilizar esses dados granularmente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9b5d26-66aa-49e1-953b-d2f13e32597b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    dataframe_dados \n",
    "        .withColumn(\"Ano\", year(\"Data\")) \n",
    "        .withColumn(\"Mes\", month(\"Data\")) \n",
    "        .write.mode(\"overwrite\").partitionBy(\"Ano\", \"Mes\").parquet(bronze_destino_dados)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36f2d92-973d-42d5-a423-e67ab7d9dac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> O modo de escrita define como os dados serão gravados no destino:\n",
    "- ***overwrite***: remove os dados existentes no caminho/partições e grava tudo novamente\n",
    "- ***append***: adiciona novos dados às partições existentes, sem apagar o que já existe \n",
    "\n",
    "> Em pipelines produtivos, 'append' é o mais comum para cargas incrementais. 'overwrite' costuma ser usado apenas em reprocessamentos completos ou ambientes de teste\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 6 - **Limpeza de Cache e Outros**\n",
    "\n",
    "O código a seguir exclui todos os Data Frames que podem estar em cache no algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c470311-85ab-4434-ae15-5e122826c94e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No Databricks Free Edition (Serverless), o gerenciamento de cache e memória é feito automaticamente pela plataforma. Por esse motivo, alguns comandos tradicionais do Spark não são suportados, como:\n",
    "\n",
    "- dataframe.unpersist()\n",
    "- spark.catalog.clearCache()\n",
    "\n",
    "A alternativa é remover referências aos DataFrames manualmente e forçar a coleta de lixo do Python quando necessário.\n",
    "\"\"\"\n",
    "\n",
    "del dataframe_dados\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799a966e-528a-4009-b5b4-01067af1acfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Resultado Final**\n",
    "\n",
    "O código a seguir monta um novo Data Frame com os dados diretamente da camada bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf9590c-de07-45e1-a664-bc21bbfe3bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "\"\"\"\n",
    "dataframe_bronze = spark.read.parquet(bronze_destino_dados)\n",
    "\n",
    "display(dataframe_bronze)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683d8bc8-4810-416f-bd6b-dda8eb7691f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "E o código a seguir faz a mesma coisa que o anterior, mas filtrando para mostrar somente mês de janeiro. Mostrando assim a performance mais rápida por ter dividido anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97255de1-a91d-4683-8024-646ac556bdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código está comentado por ser apenas uma demonstração. Para visualizar o resultado, retire o identificador de comentário e execute novamente\n",
    "\n",
    "\"\"\"\n",
    "dataframe_bronze_janeiro = (\n",
    "    spark.read\n",
    "        .parquet(bronze_destino_dados)\n",
    "        .filter(\n",
    "            (col(\"Ano\") == 2012) &\n",
    "            (col(\"Mes\") == 1)\n",
    "        )\n",
    ")\n",
    "\n",
    "display(dataframe_bronze_janeiro)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 2 - Processamento para Camada Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
