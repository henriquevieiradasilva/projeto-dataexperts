{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6ca6acf-0fce-44ad-90ec-4b9e841412cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETAPA 3 - TRANSFORMAÇÃO CAMADA SILVER**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### TODO\n",
    "- Testar funções trim e not null, e ajustar conforme necessário\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Parte 1 - **Verificação de catálogos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf60270-cce1-457d-98c1-e444f5a0d0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do catalog\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS data_ex\")\n",
    "\n",
    "# Criação do schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS data_ex.desafio\")\n",
    "\n",
    "# Criação dos volumes\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS data_ex.desafio.lhdw\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS data_ex.desafio.bronze\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS data_ex.desafio.silver\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS data_ex.desafio.gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a8190c-d136-4a42-a6a1-5797f88cee11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transformação Data Silver\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n",
    "\n",
    "# Definir caminhos de armazenamento no Data Lake\n",
    "# Ler dados na Bronze e Armazenar Silver\n",
    "\n",
    "bronze_path = \"/Volumes/data_ex/desafio/bronze\"\n",
    "silver_path = \"/Volumes/data_ex/desafio/silver\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74aeab0-37b7-4fe8-905a-4c1160554e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler dados da camada Bronze\n",
    "df_bronze = spark.read.format(\"parquet\").load(bronze_path)\n",
    "#display(df_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a124829-dc74-4de6-857d-4fe2881fff48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Testar função de trim\n",
    "for field in df_bronze.schema.fields:\n",
    "    if isinstance(field.dataType, StringType):\n",
    "        df_silver = df_bronze.withColumn(field.name, trim(col(field.name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c5b6a69-8d1c-4143-a6c8-b61a07c87c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# Realizar transformações necessárias\n",
    "df_silver = df_silver.withColumn(\"Data\", to_date(col(\"Data\"), \"yyyy-MM-dd\")) \\\n",
    "                     .withColumn(\"Email\", lower(expr(\"regexp_replace(split(EmailNome, ':')[0], '[()]', '')\"))) \\\n",
    "                     .withColumn(\"Nome\", expr(\"split(split(EmailNome, ':')[1], ', ')\")) \\\n",
    "                     .withColumn(\"Nome\", expr(\"concat(Nome[1], ' ', Nome[0])\")) \\\n",
    "                     .withColumn(\"Cidade\", expr(\"split(Cidade, ',')[0]\")) \\\n",
    "                     .withColumn(\"TotalVendas\", format_number(col(\"PrecoUnitario\") * col(\"Unidades\"),2)) \\\n",
    "                     .withColumn(\"PrecoUnitario\", format_number(col(\"PrecoUnitario\"), 2)) \\\n",
    "                     .withColumn(\"CustoUnitario\", format_number(col(\"CustoUnitario\"), 2)) \\\n",
    "                     .drop(\"EmailNome\")\\\n",
    "                     .drop(\"IdCampanha\")   \n",
    "                     \n",
    "\n",
    "display(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "575a1ca2-0cc5-4410-aa94-f8db2ec10f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Testar função Not Null\n",
    "df_silver = df_silver.filter(col(\"cst_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d04ba7e5-60f5-4b01-84ac-f860ca934760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETAPA DO BRONZE - TEMPLATE VIEW\n",
    "\n",
    "Esta função abaixo não faz parte da etapa silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca7aa47-4540-449d-bc4c-e19df3b8078d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ETAPA DO BRONZE - TEMPLATE VIEW\n",
    "\n",
    "# Criando uma lista para armazenar os Sparks DataFrames\n",
    "spark_df = []\n",
    "\n",
    "# Criação do Spark DataFrame, leitura dos csv, aplicação dos schemas e escrita em parquet \n",
    "def bronze_work(csv_file, schemas):\n",
    "    for file, schema in zip(csv_files, schemas):\n",
    "        # Tenta ler os arquivos csv em lhdw_path\n",
    "        try:\n",
    "            # Se schema não for vendas, aplica o schema correspondente a file\n",
    "            if schema is not \"schema_vendas\":\n",
    "                df = spark.read.csv(f\"{lhdw_path}/{file}\", header=True, schema=schema)\n",
    "                print(f\"Leitura do arquivo {file} bem suceddida.\")\n",
    "            \n",
    "            # Se for vendas, deve repetir a leirura do arquivo n vezes\n",
    "            else:\n",
    "                # Tenta ler o caminho no qual vendas esta\n",
    "                try:\n",
    "                    paths = dbutils.fs.ls(lhdw_path)\n",
    "                    # Conta o numero csv vendas existentes\n",
    "                    n = len([n for n in paths if n.name.startswith(\"vendas\")])\n",
    "                    # Aplica um loop para ler todos os arquivos vendas\n",
    "                    for i in range(1, n+1):\n",
    "                        try:\n",
    "                            df = spark.read.csv(f\"{lhdw_path}/vendas_part{i}.csv\", header=True, schema=schema)\n",
    "                            print(f\"Leitura do arquivo vendas_part{i}.csv bem sucedida.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erro ao ler o arquivo {lhdw_path}/vendas_part{i}.csv: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(\"ERRO\")\n",
    "                    print(f\"Erro ao ler o caminho no qual no qual vendas esta {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "bronze_work(csv_files, schemas)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
